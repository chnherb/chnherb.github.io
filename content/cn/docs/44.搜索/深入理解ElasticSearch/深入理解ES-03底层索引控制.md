---
categories: ["ES"]
tags: [""]
title: "深入理解ES-03底层索引控制"
# linkTitle: ""
weight: 10
description: >

---



## 改变Lucene评分方式

Lucene 4.0 发布后可以改变默认的基于 TF/IDF 的评分算法，因为 Lucene 的 API 做了一些改变，使得用户能轻松地修改和扩展该评分公式。且提供了更多的相似度模型，允许我们采用不同的评分公式。

### 可用的相似度模型

Lucene 4.0 之前，除了最原始和默认的相似度模型外，TF/IDF 模型也是可用的。在此基础上又新增了3种相似度模型可供使用：

* Okapi BM25 模型：基于概率模型的相似度模型，可用于估算文档与给定查询匹配的概率。模型名字：BM25。一般来说，该模型在短文本文档上的效果最好，因为这种场景中重复词项对文档的总体得分损害较大。
* 随机偏离（Divergence from randomness）模型：基于同名概率模型的相似度模型。模型名字：DFR。一般来说，随机偏离模型在类似自然语言的文本上效果较好。
* 基于信息的（information based）模型：与随机偏离模型类似。模型名字：IB。同样，IB模型也在类似自然语言的文本上拥有较好的效果。
### 为每字段配置相似度模型

ES 0.90 以后，用户可以在映射中为每字段设置不同的相似度模型。例如：

```json
{
  "mappings": {
    "post": {
      "properties": {
        "id": {
        "type": "long", 
        "store": "yes",
        "precision_step": "0"
        },
        "name": {
          "type": "string",
          "store": "yes",
          "index": "analyzed"
        },
        "contents": {
          "type": "string",
          "store": "no",
          "index": "analyzed"
        }
      }
    }
  }
}
```
如果希望在 name 字段和 contents 字段使用 BM25 相似度模型，可以添加 similarity 字段，并将该字段设置为响应的相似度模型名字。如下：
```json
{
  "mappings": {
    "post": {
      "properties": {
        "id": {
        "type": "long", 
        "store": "yes",
        "precision_step": "0"
        },
        "name": {
          "type": "string",
          "store": "yes",
          "index": "analyzed",
          "similarity": "BM25"
        },
        "contents": {
          "type": "string",
          "store": "no",
          "index": "analyzed",
          "similarity": "BM25"
        }
      }
    }
  }
}
```
以上修改就足够了，并不需要额外配置。
>注意：对于随机偏离模型和基于信息的相似度模型，需要一些额外的配置，用于控制这些相似度模型的行文。

## 相似度模型配置

### 选择默认相似度模型

提供一个名为 default 的相似度模型的配置信息，需要将配置文件修改为如下形式：

```json
{
  "settings": {
    "index": {
      "similarity": {
        "default": {
          "type": "default",
          "discount_overlaps": false
        }
      }
    }
  }
}
```
改为名为 base 的相似度模型：
```json
{
  "settings": {
    "index": {
      "similarity": {
        "base": {
          "type": "default",
          "discount_overlaps": false
        }
      }
    }
  }
}
```

### 配置被选用的相似度模型

#### 配置TF/IDF相似度模型

只设置一个参数：discount_overlaps 属性，其默认值为true。默认情况下，位置增量（position increment）为0（即该词条的position计数与前一个词条相同）的词条在计算评分时并不会被考虑进去。如果在计算文档时需要考虑这类词条，则需要将相似度模型的 discount_overlaps 属性值设置为 false。

#### 配置Okapi BM25相似度模型

有如下参数可供配置：

* k1：浮点数，控制饱和度（saturation），即词频归一化中的非线性项
* b：浮点数，用于控制文档长度对词频的影响
* discount_overlaps：与TF/IDF相似度模型的discount_overlaps参数作用相同
#### 配置DFR相似度模型

有如下参数可供配置：

* basic_model：可设置为 be、d、g、if、in 和 ine。
* after_effect：可设置为 no、b 和 l。
* normalization：可设置为 no、h1、h2、h3 和 z。
如果 normalization 参数值不是 no，则需要设置归一化因子（依赖于normalization参数值）。参数值为 h1 时，使用 normalization.h1.c 属性；参数值为 h2 时，使用 normalization.h2.c 属性；参数值为 h3 时，使用 normalization.h3.c 属性；参数值为 z 时，使用 normalization.z.z 属性。属性值的数据类型均为浮点型，如下：

```json
"similarity": {
  "esserverboook_dfr_similarity": {
    "type": "DFR",
    "basic_model": "g"，
    "after_effect": "l",
    "normalization": "h2",
    "normalization.h2.c": "2.0"
  }
}
```

#### 配置IB相似度模型

有如下参数可供配置：

* distribution：可设置为 ll 或 spl。
* lambuda：可设置为 df 或 tff。
IB 模型也需要配置归一化因子，配置方式与 DFR 模型相同。如下：

```json
"similarity": {
  "esserverboook_ib_similarity": {
    "type": "IB",
    "basic_model": "ll"，
    "after_effect": "df",
    "normalization": "z",
    "normalization.z.z": "0.25"
  }
}
```

## 使用编解码器

Lucene 4.0 的一个显著变化是允许用户改变索引文件编码方式。在此之前之前只能通过修改内核代码来实现。它提供了灵活的索引方式，允许用户改变倒排索引的写入方式。

### 简单使用范例

需要改变索引写入格式的理由之一是性能。某些字段需要特殊处理，如主键，借助一些技术，主键值能很快被搜索到。还可以使用 SimpleTextCode 来调试代码，以便了解写入索引中的数据格式。

>编解码器是 Lucene 提供的功能，ES 并没有相应的接口。

### 工作原理解释

假设 posts 索引有如下映射：

```json
{
  "mappings": {
    "post": {
      "properties": {
        "id": {
          "type": "long", 
          "store": "yes",
          "precision_step": "0"
        },
        "name": {
          "type": "string", 
          "store": "yes",
          "index": "analyzed"
        },
        "contents": {
          "type": "string",
          "store": "no",
          "index": "analyzed"
        }
      }
    }
  }
}
```
编解码器需要逐字段配置。为了配置某个字段使用特定的编解码器，需要在字段配置文件中添加 postings_format 属性，并赋值如 plusing。如下：
```json
{
  "mappings": {
    "post": {
      "properties": {
        "id": {
          "type": "long", 
          "store": "yes",
          "precision_step": "0",
          "postings_format": "plusing"
        },
        "name": {
          "type": "string", 
          "store": "yes",
          "index": "analyzed"
        },
        "contents": {
          "type": "string",
          "store": "no",
          "index": "analyzed"
        }
      }
    }
  }
}
```
然后执行以下命令，查看配置是否生效（id字段的编码器类型）：
```shell
curl -XGET 'localhost:9200/posts/_mapping?pretty'
```

### 可用的倒排表格式

* defult：当没有显式配置时，倒排表使用该格式。该格式提供了存储字段（stored field）和词项向量压缩功能。
* plusing：该编码器将高基（high cardinality）字段中的倒排表编码为词项数组，会减少 Lucene 搜索文档时的查找操作。使用该编码器，可以提高在高基字段中的搜索速度。
* direc：该编码器在读索引阶段将词项载入词典，且词项在内存汇总为未压缩状态。该编码器能提升常用字段的查询性能。词项和倒排表数组都需要存储在内存中，非常消耗内存，谨慎使用。
* memory：将所有数据写入磁盘，读取时则使用 FST（Finite State Transducers）结构直接将词项和倒排表载入内存。数据都在内存因而能加速常见词项的查询。
* bloom_default：是 default 编解码器的一种扩展，在这基础上加入了 bloom filter 的处理，且 bloom filter 相关数据会写入磁盘中。当读入索引时，bloom filter 相关数据会被读入内存，用于快速判断某个特定值是否存在。
* bloom_pulsing：pulsing 编解码器的扩展，在其基础上又加入 bloom filter 的处理。
### 配置编解码器

custom_default

```json
{
  "settings": {
    "index": {
      "codec": {
        "postings_format": {
          "custom_default": {
            "type": "default",
            "min_block_size": "20",
            "max_block_size": "60"
          }
        }
      }
    }
  }
  "mappings": {
    "post": {
      "properties": {
        "id": {
          "type": "long", 
          "store": "yes",
          "precision_step": "0"
        },
        "name": {
          "type": "string", 
          "store": "yes",
          "index": "analyzed",
          "postings_format": "custom_default"
        },
        "contents": {
          "type": "string",
          "store": "no",
          "index": "analyzed"
        }
      }
    }
  }
}
```

#### default编解码器属性

* min_block_size：确定了 Lucene 将词项词典（term dictionary）中的多个词项编码为块（block）时，块中的最小词项数。默认值为25。
* max_block_size：同上，块中的最大词项数。默认值为48。
#### direct编解码器属性

* min_skip_count：确定允许写入跳表（skip list）指针的具有相同前缀的词项的最小数量。默认值为8。
* low_freq_cutoff：编解码器使用单个数组对象来存储文档频率（document frequence）低于该参数值的词项的倒排链及位置信息。默认值为12。
#### memory 编解码器属性

* pack_fst：布尔乐行，默认设置为false，用来确认保存倒排链的内存结构是否被打包为 FST（Finite State Transducers）类型。而打包为 FST 类型能减少保存数据所需的内存量。
* acceptable_overhead_ratio：浮点型，指定了内部结构的压缩率，默认值为0.2。值为0时，没有额外的内存消耗，但会导致较低的性能。值为0.5时，将会多付出50%的内存消耗，但能提升性能。值超过1也是可行的，但会导致更多的内存开销。
#### pulsing编解码器属性

除了 default 编解码器的属性外，还有如下：

* freq_cut_off：默认为1，表示文档频率阈值，若词项对应的文档频率小于等于该阈值，则将该词项的倒排链写入词典中。
#### bloom filter编码器属性

* delegate：用来确定将要被 bloom filter 包装（wrap）的编解码器
* ffp：介于0与1.0之间，用来确定期望的假阳率（false positive probability）。可以依据每个索引段中的文档树设置多个ffp值。例如，默认情况下 10k 个文档时为 0.1，1m 个文档时为 0.03。表示当索引端中文档数大于 10 000 个时 ffp 值使用 0.01，而当文档数超过 1 000 000 时，ffp 值使用 0.03。
## 准实时、提交、更新及事务日志

### 索引更新及更新提交

索引期新文档会写入索引段，索引段是独立的 Lucene 索引，这意味查询可以和索引并行，只是不时会有新增的索引段被添加到可被搜索的索引段集合之中。Lucene 通过创建后续的（基于索引只写一次的特性）segments_N 文件来实现此功能，且该文件列举了索引中的索引段。该过程成为提交（committing），Lucene 以一种安全的方式来执行该操作，能确保索引更改以原子操作方式写入索引，即便发生错误，也能保证索引数据的一致性。

一次提交并不足以保证新索引的数据都能被搜索到，Lucene 使用了一个叫作 Searcher 的抽象类来执行索引的读取。如果索引更新提交了，但 Searcher 实例并没有重新打开，它察觉不到新索引段的加入。Searcher 重新打开的过程叫作刷新（refresh）。考虑性能 Lucene 推迟了耗时的刷新，不会在每次新增一个文档（或批量增加文档）时刷新，但 Searcher 会每秒刷新一次。这已经很频繁了，但有些应用仍然需要更快的刷新频率，需要使用其它技术或评估是否合理。ES 提供了强制刷新的 API，在搜索前执行该命令即可，如：

```shell
curl -XGET localhost:9200/test/_refresh
```

#### 更新默认的刷新时间

Searcher 自动刷新时间间隔可以通过以下手段改变：更改 ES 配置文件中的 index.refresh_interval 参数值或者使用配置更新相关的 API。例如：

```shell
curl -XPUT localhost:9200/test/_setting -d '{
  "index": {
    "refresh_interval": "5m"
  }
}'
```
该命令将 Searcher 自动刷新时间间隔更改为 5 分钟。
>刷新操作很消耗资源，因此刷新间隔时间越长，索引速度越快。如果需要长时间高速建索引，并在建索引结束之前暂不执行查询，可以考虑将 index.refresh_interval 设置为 -1，然后在建索引结束后再将该参数恢复为初始值。

### 事务日志

ES 通过使用事务日志（transaction log）来解决写数据失败的问题（磁盘空间不足、设备损坏、没有足够的文件句柄共索引文件使用等），它能保证所有的未提交的事务，而 ES 会不时创建一个新的日志文件用于记录每个事务的后续操作。当有错误发生时就会检查事务日志，必要时会再次执行某些操作，以确保没有提示任何更改信息。且事务日志的相关操作都是自动完成的，用户并不会意识到某个特定时刻触发的更新提交。事务日志中的信息与存储介质之间的同步（同时清空事务日志）称为事务日志刷新（flushing）。

>注意事务日志刷新与 Searcher 刷新的区别。

除了自动的事务日志刷新以外，还可以使用对应的 API，如使用下面的命令强制将事务日志涉及的所有数据更改操作同步到索引中，并清空事务日志文件：

```shell
curl -XGET localhost:9200/_flush
```
也可以使用 flush 命令对特定的索引进行事务日志刷新（如 library 索引）：
```shell
curl -XGET localhost:9200/library/_flush
curl -XGET localhost:9200/library/_refresh # 紧接着调用Searcher刷新操作，打开一个新Searcher实例
```
#### 事务日志相关配置

以下参数通过修改 elasticsearch.yml 文件来配置，也可以通过索引配置更新 API 来更改：

* index.translog.flush_threshold_period：默认值为30分钟，控制了强制自动事务日志刷新的时间间隔，即便没有新数据写入。强制进行事务日志刷新通常会导致大量的 I/O 操作，因此当事务日志涉及少量数据时，才更适合进行这项操作。
* index.translog.flush_threshold_ops：确定了一个最大操作数，即在上次事务日志刷新以后，当索引更改操作次数超过该参数时，强制进行事务日志刷新操作，默认值为 5000。
* index.translog.flush_threshold_size：确定了事务日志的最大容量，当容量大于等于该参数值，就强制进行事务日志刷新操作，默认值为 200MB。
* index.translog.disable_flush：禁用事务日志刷新。临时性禁用能带来其它便利，如向索引中导入大量文档。
除了修改 elasticsearch.yml 文件的方式，也可以通过设置更新 API 该更改相关配置。如：

```shell
curl -XPUT localhost:9200/test/_settings -d '{
  "index": {
    "translog.disable_flush": true
  }
}'
```

### 准实时读取

事务日志带来一个免费特性：实时读取（real-time GET），该功能让返回文档各种版本（包括未提交版本）成为可能。实时读取操作从索引中读取数据时，会先检查事务日志中是否有可用的新版本。如果近期索引没有与事务日志同步，那么索引中的数据将会被忽略，事务日志中的最新版本的文档将会被返回。如下命令：

```shell
curl -XGET localhost:9200/test/test/1?pretty
```

## 深入理解数据处理

### 输入并不总是文本分析

创建索引：

```shell
curl -XPUT localhost:9200/etst -d '{
  "mappings": {
    "test": {
      "properties": {
        "title": {
          "type": "string",
          "analyzer": "snowball"
        }
      }
    }
  }
}'
```
添加文档：
```shell
curl -XPUT localhost:922/test/test/1 -d '{
  "title": "the quick brown fox jumps over the lazy dog"
}'
```
测试命令：
```shell
curl localhost:9200/test/_search?pretty -d '{
  "query": {
    "term": {
      "title": "jumps"
    }
  }
}'
curl localhost:9200/test/_search?pretty -d '{
  "query": {
    "match": {
      "title": "jumps"
    }
  }
}'
```
第一个查询返回了目标文档，而第二个查询没有返回任何结果。该现象与文本分析有关。
通过下面命令来使用文本分析（Analyze）API：

```shell
curl 'localhost:9200/test/_analyze?text=the+quick+brown+fox+jumps+over+the+lazy+dog&pretty&analyzer=snowball'
```
端点 _analyze 允许查看 ES 是如何处理 text 参数中的输入，也能指定要使用哪个分析器（通过 analyzer 参数）。
上面命令的返回结果会发现：每个词条都携带了在原始文本中的位置信息、类型信息（过滤器可能用到）、词项信息（一个词存储在索引中，在检索其用于与查询中的词项匹配）。原始文本被转换成了这些词项：quick、brown、fox、jump、over、lazi（有趣的变化）、dog。总计 snowball 分词器做了哪些事情：

* 过滤非重要次（如 the）
* 将单词转换为词干形式（如 jump）
* 有时会进行操作的转换（如 lazi）
第二个测试范例：

```shell
curl localhost:9200/test/_search?pretty -d '{
  "query": {
    "prefix": {
      "title": "lazy"
    }
  }
}'
curl localhost:9200/test/_search?pretty -d '{
  "query": {
    "match_phrase_prefix": {
      "title": "lazy"
    }
  }
}'
```
查询结果：第一个没有返回（因为查询中的 lazy 文本与索引中的 lazi 并不相同），而第二个查询经过分词器处理，返回了预期的文档。
### 范例的使用

从概率的角度来看，与查询短语在文本上精确撇配的文档应该是用户最感兴趣的。从另一个重要指标来看，与用户输入短语中词语精确匹配的文档才是用户感兴趣的。这里的词语精确匹配既可以是语义上相同也可以是同一个词的不同形态。

创建一个只包含单字段文档的索引：

```shell
curl -XPUT localhost:9200/test -d '{
  "mappings": {
    "test": {
      "properties": {
        "lang": {
          "type": "string"
        },
        "title": {
          "type": "multi_field",
          "fields": {
            "i18n": {
              "type": "string",
              "index": "analyzed",
              "analyzer": "english"
            },
            "org": {
              "type": "string",
              "index": "analyzed",
              "analyzer": "standard"
            }
          }
        }
      }
    }
  }
}'
```
一个字段使用了两个分析器进行文本分析处理，因为 title 字段是 multi_field 类型。其中对 title.org 子字段使用了 standard 分析器，而对 title.i18n 子字段使用了 english 分析器（该分词器会将用户输入转换为词干形式）。
添加文档：

```shell
curl -XPUT localhost:9200/test/test/1 -d '{"title": "The quick brown fox jumps over the lazy dog."}'
```
此时，索引中的 title.dog 字段已有 jumps 词项，而 title.i18n 字段也有了jump词项。执行以下查询：
```shell
curl localhost:9200/test/_search?pretty -d '{
  "query": {
    "multi_match": {
      "query": "jumps",
      "fields": ["title.org^1000", "title.i18n"]
    }
  }
}'
```
文档由于跟查询完美匹配获得了较高的得分，归功于对 field.org 字段的命中做了加权处理。field.i18n 字段的命中也贡献了部分得分，只是对总得分影响小很多，因为其默认权值为1。
### 索引期更换分词器

```shell
curl -XPUT localhost:9200/test -d '{
  "mappings": {
    "test": {
      "_analyzer": {
        "path": "lang"
      }
      "properties": {
        "lang": {
          "type": "string"
        },
        "title": {
          "type": "multi_field",
          "fields": {
            "i18n": {
              "type": "string",
              "index": "analyzed",
            },
            "org": {
              "type": "string",
              "index": "analyzed",
              "analyzer": "standard"
            }
          }
        }
      }
    }
  }
}'
```
允许 ES 在处理文本时根据文本内容决定采用何种分析器。path 参数为文档中的字段名，该字段保存了分析器的名称。其次移除了 field.i18n 字段所用分析器的定义。
创建索引：

```shell
curl -XPUT localhost:9200/test/test/1 -d '{"title": "The quick brown fox jumps over the lazy dog.", "lang": "english"}'
```
这个例子 ES 从索引中提取 lang 字段的值，并将该值代表的分析器置于当前文档的文本分析器处理。如在文档中移除或保留非重要词等场景非常有用。
### 搜索时更换分析器

也可以在搜索时更换分词器，并通过配置 analyzer 属性来实现。如：

```shell
curl localhost:9200/test/_search?pretty -d '{
  "query": {
    "multi_match": {
      "query": "jumps",
      "fields": ["title.org^100", "title.i18n"],
      "analyzer": "english"
    }
  }
}'
```

### 陷进与默认分析

索引期与检索期能针对文档更换分词器的机制是一个非常有用的特性，但也会引入非常隐蔽的错误。如没有定义分析器。这种情况 ES 会选用一个默认分析器，但这不是我们期望的。因为默认分析器有时候会被文本分析插件模块重定义。此时有必要指定默认分析器，如将自定义的分析器名称替换为default。

>备选方案，定义 default_index 分析器和 default_search 分析器。

## 控制索引合并

ES 每个索引都会创建一到多个分片以及零到多个副本，本质上都是 Lucene 索引（基于多个索引端构建，至少一个）。索引文件绝大部分数据都是只写一次、读多次，自由用于保存文档删除信息的文件才会被多次更改。某种情况满足条件时，多个索引段会被拷贝合并到一个更大的索引段，而旧的索引段会被抛弃并从磁盘中删除，这个操作称为段合并（segment merging）。

段合并的原因：

1、索引段个数越多，搜索性能越低且耗费内存更多。

2、索引段是不可变的，物理上并不能从中删除信息。

3、碰巧索引中删除了大量文档，但文档只做了删除标记，物理机上没有被删除。段合并时，标记为删除的文档并没有复制到新的索引段中。因此减少了最终索引端中的文档数。

从用户角度段合并可概括为：

1、当多个索引端合并为一个时，会减少索引段的数量并提高搜索速度。

2、同时也会减少索引的容量（文档数），因为在段合并时会移除被标记为已删除的文档。

段合并的代价主要是 I/O 操作。在速度较慢的系统中会显著影响性能。因此 ES 允许用户选择段合并策略（merge policy）及存储级节流（store level throttling）

### 选择正确的合并策略

三种可用的合并策略：

* tiered（默认）
* log_byte_size
* log_doc
配置文件的 index.merge.policy.type 字段：

```plain
index.merge.policy.type: tiered
```
>一旦使用特定的段合并策略创建了索引，就不能被改变。但是可以使用索引更新API来改变该段合并策略的参数值。

#### tiered合并策略

ES 默认选项。能合并大小相似的索引段，并考虑每层允许的索引段的最大个数。需要注意单次可合并的索引段的个数与每层允许的索引段数的区别。

索引期，合并策略会计算索引中允许出现的索引段个数，称为阈值（budget）。如果正在构建的索引中的段数超过了阈值，该策略将先对索引段按容量降序排序（这里考虑了被标记为已删除的文档），然后再选择一个成本最低的合并。合并成本的计算方法倾向于回收更多删除文档和产生更小的索引段。

如果某次合并产生的索引段的大小大于 index.merge.policy.max_merged 参数值，则该合并策略会选择更少的索引段参与合并，是的生成的索引段大小小于阈值。即对于有多个分片的索引，默认的 index.merge.policy.max_merged 则显得过小，会导致大量索引段的创建，从而降低查询速度。用户应根据具体的数据量观察索引段的情况，不断调整合并策略以满足需求。

#### log byte zise合并策略

该策略会不断地以字节数的对数为计算单位，选择多个索引来合并创建新索引。合并过程中，会出现一些较大的索引段，然后又产生出一些小于合并因子（merge factor）的索引段，如此循环往复。可以想象一些相同数量级的索引段，其个数会变得比合并因子还少。对一些特别大的索引段，所有小于该级别的索引段都会被合并。索引中的索引段个数与下次用于计算的字节数的对数成正比。因此，该合并策略能够保持较少的索引段数量并且极少化段索引合并的代价。

#### log doc合并策略

与 log_byte_size 合并策略类似，不同的是 log_byte_size 基于索引的字节数计算，而 log_doc 基于索引段的文档数计算。以下两种情况该合并策略表现良好：

* 文档集中的文档大小类似
* 参数合并的索引段在文档数方面相当
### 合并策略配置

大多数情况下默认选项是够用的，除非有特殊的需求才需要修改。

#### 配置tiered合并策略

* index.merge.policy.expunge_deletes_allowed：默认值为0，用于确定被删除文档的百分比，当执行 expungeDeletes 时，该参数用于确定索引段是否被合并。
* index.merge.policy.floor_segment：用于阻止频繁刷新微小索引段。小于该值的索引段由索引合并机制处理，并将这些索引段的大小作为该参数值。默认值为 2MB。
* index.merge.policy.max_merge_at_once：确定了索引期单次合并涉及的索引段数量的上限，默认为10.该值较大时，允许更多的索引段参与单次合并，只是会消耗更多的 I/O 资源。
* index.merge.policy.max_merge_at_once_explicit：确定索引优化（optimize）操作和expungeDeletes 操作能参与的索引段数量的上限，默认值为30。但该值对索引期参与合并的索引段数量的上限没有影响。
* index.merge.policy.max_merged_segment：默认值为 5GB，确定了索引期单次合并中产生的索引段大小的上限。这是一个近似值，因为合并后产生的索引段的大小是通过累加参与合并的索引段的大小并减去被删除文档的大小而得来的。
* index.merge.policy.segments_per_tier：确定每层允许出现的索引段数量的上限。越小的参数值会导致更少的索引段数量，意味更多的合并操作以及更低的索引性能。默认值为10，建议设置为大于等于 idnex.merge.policy.max_merge_at_once，否则会遇到很多与索引合并以及性能相关的问题。
* index.reclaim_deletes_weight：默认为 2.0，确定了索引合并操作中清除被删除文档这个因素的权重。如果设置为 0.0，则清除被删除文档对索引合并并没有影响。该值越高，则清除较多被删除文档的合并会更受合并策略青睐。
* index.compund_format：布尔型，确定了索引是否存储为复合文件格式（compound format），默认值为 false。true 则 Lucene 会将所有文件存储在一个文件中。这样设置有时能解决操作系统打开文件处理器过多的问题，但也会降低索引和搜索的性能。
* index.merge.async：布尔型，用来确定索引合并是否异步进行。默认为 true。
* index.merge.async_interval：当 index.merge.async 设置为 true 时（异步进行），该值确定了两次合并的时间间隔，默认为 1s。为了触发真正的索引合并以及索引段数量缩减操作，该值应该保持为一个较小值。
#### 配置log byte size合并策略

* merge_factor：确定索引期间索引段以多大的频率进行合并。值越小搜索的速度越快，消耗的内存也越小，而代价则是更慢的索引速度。反之亦然。默认为 10，对于批量索引构建，可以设置较大的值，对于日常索引维护则可采用默认值。
* min_merge_size：定义了索引段可能的最小容量（段中所有文件的字节数）。如果索引段大小小于该参数值，且 merge_fator 参数值允许，则进行索引段合并。该参数默认值为 1.6MB，对于避免产生大量小索引段非常有用。设置较大值时，会导致较高的合并成本。
* max_merge_size：定义了允许参与合并的索引段的最大容量（以字节为单位）。默认不做设置，因而在索引合并时对索引段大小没有限制。
* maxMergeDocs：定义了参与合并的索引段的最大文档数。默认没有设置，因此当索引合并时对索引段没有最大文档数的限制。
* calibrate_size_by_deletes：该参数为布尔值，如果设置为 true，则段中被删除文档的大小会用于索引段大小的计算。
* index.compund_format：布尔值，确定了索引文件是否存储为复合文件格式，默认为 false。可参考 tiered 合并策略配置中该选项的解释。
#### 配置log doc合并策略

* merge_factor：同上
* min_merge_docs：定义饿了最小索引段允许的最小文档数。如果某索引段的文档数低于该参数值，且 merge_factor 参数允许，就会执行索引合并。该参数默认值为 1000，对于避免产生大量小索引段非常有用。过大会增大索引合并的代价。
* max_merge_docs：定义了可参与索引合并的索引段的最大文档数。默认情况下，该参数没有设置，因而对参与索引合并的索引段的最大文档数没有限制。
* calibrate_size_by_deletes：布尔值，设置为 true，则段中被删除文档的大小会在计算索引段大小时考虑进去。
* index.compund_format：布尔值，确定了索引文件是否存储为复合文件格式，默认为 false。同上。
### 调度

除了可以影响索引合并策略的行为之外，ES 还允许定制合并策略的执行方式。索引合并调度器（scheduler）分为两种，默认的是并发合并调度器 ConcurrentMergeScheduler。

#### 并发合并调度

该调度器使用多线程执行索引合并操作。具体过程是：每次开启一个新线程知道线程数达到上限，当达到上限时，必须开启新线层（因为需要进行新的段合并），那么所有索引操作将被挂起，直到至少一个索引合并操作完成。

为了控制最大线程数，可以通过修改 index.merge.scheduler.max_thread_count 属性来实现。一般可以按照如下公式来计算允许的最大线程数：

```plain
maximum_value(1, minimum_value(3, available_processors / 2))
```
如何系统是 8 核，那么调度器允许的最大线程数可以设置为 4。
#### 顺序合并调度

使用同一个线程执行所有的索引合并操作。执行合并时，该线程的其它文档处理都会被挂起，从而索引操作会延迟进行。

#### 设置合并调度

为了设置特定的索引合并调度器，用户可将 index.merge.scheduler.type 的属性值设置为 concurrent 或 serial。如使用并发合并调度器：

```plain
index.merge.scheduler.type: concurrent
```
使用顺序合并调度器：
```plain
index.merge.scheduler.type: serial
```

## 小结

本章学习了如何使用不同的评分公式及其好处，也了解了不同的倒排索引格式及其优点。还介绍了准实时搜索和实时读取以及 Searcher 刷新对 ES 的意义。另外还讨论了多语言数据处理和如何按需配置事务日志，最后介绍了索引的段合并、合并策略以及调度。

