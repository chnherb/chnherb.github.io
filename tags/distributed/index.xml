<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Herbdocs – distributed</title><link>/tags/distributed/</link><description>Recent content in distributed on Herbdocs</description><generator>Hugo -- gohugo.io</generator><atom:link href="/tags/distributed/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: 分布式一致性算法</title><link>/docs/41.%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/41.%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7%E7%AE%97%E6%B3%95/</guid><description>
&lt;h1 id="基本概念">基本概念&lt;/h1>
&lt;h2 id="cap">CAP&lt;/h2>
&lt;p>C：一致性&lt;/p>
&lt;p>A：可用性&lt;/p>
&lt;p>P：分区容错性&lt;/p>
&lt;p>CP或者AP&lt;/p>
&lt;h2 id="base-理论">&lt;strong>BASE 理论&lt;/strong>&lt;/h2>
&lt;p>BASE理论指的是基本可用 Basically Available，软状态Soft State，最终一致性 Eventual Consistency，核心思想是即便无法做到强一致性，但应该采用适合的方式保证最终一致性。&lt;/p>
&lt;p>BASE，Basically Available Soft State Eventual Consistency 的简写：&lt;/p>
&lt;p>&lt;strong>BA&lt;/strong>：Basically Available 基本可用，分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用。&lt;/p>
&lt;p>&lt;strong>S&lt;/strong>：Soft State 软状态，允许系统存在中间状态，而该中间状态不会影响系统整体可用性。&lt;/p>
&lt;p>&lt;strong>E&lt;/strong>：Consistency 最终一致性，系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。&lt;/p>
&lt;p>BASE 理论本质上是对 CAP 理论的延伸，是对 CAP 中 AP 方案的一个补充。&lt;/p>
&lt;h2 id="xa简单介绍">XA简单介绍&lt;/h2>
&lt;p>XA是由X / Open发布的规范，用于DTP（分布式事务处理）。&lt;/p>
&lt;p>DTP分布式模型主要含有&lt;/p>
&lt;ul>
&lt;li>AP： 应用程序&lt;/li>
&lt;li>TM: 事务管理器&lt;/li>
&lt;li>RM: 资源管理器(如数据库)&lt;/li>
&lt;li>CRM: 通讯资源管理器（如消息队列）&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>XA主要就是TM和RM之间的通讯桥梁。&lt;/strong>&lt;/p>
&lt;h2 id="分布式一致性算法前提">分布式一致性算法前提&lt;/h2>
&lt;h3 id="拜占庭将军问题">拜占庭将军问题&lt;/h3>
&lt;p>士兵（信息通道）可以伪造虚假信息&lt;/p>
&lt;h2 id="分布式一致性算法考虑的原则">&lt;strong>分布式一致性算法考虑的原则&lt;/strong>&lt;/h2>
&lt;ul>
&lt;li>safety : 在非拜占庭的情况下，不会返回错误的结果&lt;/li>
&lt;li>available: 在大多数server运行且可以和别的server以及client通信的情况下，系统保证可用&lt;/li>
&lt;li>不依赖时间来保证一致性，时钟错误，信息延迟会导致可用性不好&lt;/li>
&lt;/ul>
&lt;h1 id="2pc-二阶段提交">2PC 二阶段提交&lt;/h1>
&lt;p>两阶段提交协议（The two-phase commit protocol，2PC）是 XA 用于在全局事务中协调多个资源的机制&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_consistency_211219_1.png" alt="distributed_consistency_211219_1.png">&lt;/p>
&lt;h2 id="1-阶段一提交事务请求">1、阶段一提交事务请求&lt;/h2>
&lt;p>1、协调者向所有的参与者节点发送事务内容,询问是否可以执行事务操作,并等待其他参与者节点的反馈&lt;/p>
&lt;p>2、各参与者节点执行事务操作&lt;/p>
&lt;p>3、各参与者节点反馈给协调者,事务是否可以执行&lt;/p>
&lt;h2 id="2-阶段二事务提交">2、阶段二事务提交&lt;/h2>
&lt;p>根据一阶段各个参与者节点反馈的ack如果所有参与者节点反馈ack,则执行事务提交,否则中断事务&lt;/p>
&lt;h3 id="事务提交">事务提交:&lt;/h3>
&lt;p>1、协调者向各个参与者节点发送commit请求&lt;/p>
&lt;p>2、参与者节点接受到commit请求后,执行事务的提交操作&lt;/p>
&lt;p>3、各参与者节点完成事务提交后,向协调者返送提交commit成功确认消息&lt;/p>
&lt;p>4、协调者接受各个参与者节点的ack后,完成事务commit&lt;/p>
&lt;h3 id="中断事务">中断事务:&lt;/h3>
&lt;p>1、发送回滚请求&lt;/p>
&lt;p>2、各个参与者节点回滚事务&lt;/p>
&lt;p>3、反馈给协调者事务回滚结果&lt;/p>
&lt;p>4、协调者接受各参与者节点ack后回滚事务&lt;/p>
&lt;h2 id="二阶段提交存在的问题">二阶段提交存在的问题:&lt;/h2>
&lt;h3 id="1-同步阻塞">1、同步阻塞&lt;/h3>
&lt;p>二阶段提交过程中,所有参与事务操作的节点处于同步阻塞状态,无法进行其他的操作&lt;/p>
&lt;h3 id="2-单点问题">2、单点问题&lt;/h3>
&lt;p>一旦协调者出现单点故障,无法保证事务的一致性操作&lt;/p>
&lt;h3 id="3-数据不一致">3、数据不一致&lt;/h3>
&lt;p>如果分布式节点出现网络分区,标些参与者未收到Commit提交命令。则出现部分参与者完成数据提交。未收到commit的命令的参与者则无法进行事务提交,整个分布式系统便出现了数据不一致性现象。&lt;/p>
&lt;p>总结三种场景：协调者挂了；参与者挂了；两者都挂了。问题：不知道执行到哪一步才挂的，事物是否提交。所以多增加一个阶段提交，这样好歹知道是在事物提交之前挂的。&lt;/p>
&lt;p>2PC不考虑超时回滚的情况下它是安全的, 2PC可以保证safety但是不保证liveness.&lt;/p>
&lt;h1 id="3pc-三阶段提交">3PC 三阶段提交&lt;/h1>
&lt;p>3PC是2PC的改进版,实质是将2PC中提交事务请求拆分为两步,形成了anCommit、PreCommit、doCommit三个阶段的事务一致性协议&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_consistency_211219_2.png" alt="distributed_consistency_211219_2.png">&lt;/p>
&lt;h2 id="阶段一-cancommit">阶段一: CanCommit&lt;/h2>
&lt;p>1、事务询问&lt;/p>
&lt;p>2、各参与者节点向协调者反馈事务询问的响应&lt;/p>
&lt;h2 id="阶段二-preccommit">阶段二: PreCcommit&lt;/h2>
&lt;p>根据阶段一的反馈结果分为两种情况&lt;/p>
&lt;h3 id="1-执行事务预提交">1、执行事务预提交&lt;/h3>
&lt;p>1）发送预提交请求&lt;/p>
&lt;p>协调者创沂有参与者节点发送preCcommit请求,进入prepared阶段&lt;/p>
&lt;p>2）事务预提交&lt;/p>
&lt;p>各参与者节点接受到preCommit请求后,执行事务操作&lt;/p>
&lt;p>3)各参与者节点向协调者反馈事务执行&lt;/p>
&lt;h3 id="2-中断事务">2、中断事务&lt;/h3>
&lt;p>任意一个参与者节点反馈给协调者响应No时,或者在等待超时后,协调者还未收到参与者的反&lt;/p>
&lt;p>馈,就中断事务,中断事务分为两步:&lt;/p>
&lt;p>1)协调者向各个参与者节点发送abort请求&lt;/p>
&lt;p>2)参与者收到abort请求,或者等待超时时间后,中断事务&lt;/p>
&lt;h2 id="阶段三-docommit">阶段三: doCommit&lt;/h2>
&lt;h3 id="1-执行提交">1、执行提交&lt;/h3>
&lt;ol>
&lt;li>发送提交请求&lt;/li>
&lt;/ol>
&lt;p>协调者向所有参与者节点发送doCommit请求&lt;/p>
&lt;ol start="2">
&lt;li>事务提交&lt;/li>
&lt;/ol>
&lt;p>各参与者节点接受到doCommit请求后,执行事务提交操作&lt;/p>
&lt;ol start="3">
&lt;li>反馈事务提交结果&lt;/li>
&lt;/ol>
&lt;p>各参与者节点完成事务提交以后,向协调者发送ack&lt;/p>
&lt;ol start="4">
&lt;li>事务完成&lt;/li>
&lt;/ol>
&lt;p>协调者接受各个参与者反馈的ack后,完成事务&lt;/p>
&lt;h3 id="2-中断事务-1">2、中断事务&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>参与者接受到abort请求后,执行事务回滚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>参与者完成事务回滚以后,向协调者发送ack&lt;/p>
&lt;/li>
&lt;li>
&lt;p>协调者接受回滚ack后,回滚事务&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="三阶段解决的问题">三阶段解决的问题&lt;/h2>
&lt;p>1、引入超时时间，解决了同步阻塞&lt;/p>
&lt;p>2、参与者长时间接收不到协调者响应，就会将事务进行提交，使用这个机制解决了单点问题&lt;/p>
&lt;p>3PC是2PC的改进版, 实质是将2PC中提交事务请求拆分为两步, 形成了CanCommit、PreCommit、doCommit三个阶段的事务一致性协议（减少同步阻塞的发生范围）&lt;/p>
&lt;p>&lt;strong>简单概括一下就是，如果挂掉的那台机器已经执行了commit，那么协调者可以从所有未挂掉的参与者的状态中分析出来，并执行commit。如果挂掉的那个参与者执行了rollback，那么协调者和其他的参与者执行的肯定也是rollback操作。&lt;/strong>&lt;/p>
&lt;p>所以，再多引入一个阶段之后，3PC解决了2PC中存在的那种由于协调者和参与者同时挂掉有可能导致的数据一致性问题。&lt;/p>
&lt;h3 id="3pc存在的问题">3PC存在的问题&lt;/h3>
&lt;p>在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。&lt;/p>
&lt;p>所以，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况&lt;/p>
&lt;p>参考：&lt;a href="https://blog.csdn.net/yyd19921214/article/details/68953629?utm_source=app&amp;amp;app_version=4.6.1">分布式事务中2PC与3PC的区别&lt;/a>&lt;/p>
&lt;h1 id="pc的应用">PC的应用&lt;/h1>
&lt;h2 id="注意">注意&lt;/h2>
&lt;p>一般的主从同步并不涉及相关的分布式协议，基本都是异步使用日志或镜像进行主从同步，达成最终一致性。比如：DHFS的namenode，redis的主从同步以及MySQL的主从同步。不要把2pc，3pc，paxos等分布式一致性算法想象为处理主从复制这个过程的算法。&lt;/p>
&lt;h2 id="mysql的2pc应用">mysql的2pc应用&lt;/h2>
&lt;p>参考：&lt;a href="https://www.cnblogs.com/hustcat/p/3577584.html">https://www.cnblogs.com/hustcat/p/3577584.html&lt;/a>&lt;/p>
&lt;h2 id="flink的2pc应用">flink的2pc应用&lt;/h2>
&lt;p>参考：&lt;a href="https://www.cnblogs.com/zhipeng-wang/p/14082806.html">https://www.cnblogs.com/zhipeng-wang/p/14082806.html&lt;/a>&lt;/p>
&lt;h1 id="补偿事务-tcc">补偿事务（TCC）&lt;/h1>
&lt;p>TCC（&lt;strong>Try-Confirm-Cancel&lt;/strong>）又称补偿事务。其核心思想是：&amp;quot;针对每个操作都要注册一个与其对应的确认和补偿（撤销操作）&amp;quot;。它分为三个操作：&lt;/p>
&lt;ul>
&lt;li>Try阶段：主要是对业务系统做检测及资源预留。&lt;/li>
&lt;li>Confirm阶段：确认执行业务操作。&lt;/li>
&lt;li>Cancel阶段：取消执行业务操作。&lt;/li>
&lt;/ul>
&lt;p>TCC是应用层的2PC实现&lt;/p>
&lt;p>TCC事务的处理流程与2PC两阶段提交类似，不过2PC通常都是在跨库的DB层面，而TCC本质上就是一个应用层面的2PC，&lt;strong>需要通过业务逻辑来实现&lt;/strong>。这种分布式事务的实现方式的优势在于，可以让&lt;strong>应用自己定义数据库操作的粒度，使得降低锁冲突、提高吞吐量成为可能&lt;/strong>。&lt;/p>
&lt;p>而不足之处则在于&lt;strong>对应用的侵入性非常强&lt;/strong>，业务逻辑的每个分支都需要实现try、confirm、cancel三个操作。此外，其实现难度也比较大，需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。为了满足一致性的要求，&lt;strong>confirm和cancel接口还必须实现幂等&lt;/strong>。&lt;/p>
&lt;p>TCC的具体原理图如👇：&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_consistency_211219_3.png" alt="distributed_consistency_211219_3.png">&lt;/p>
&lt;h2 id="2pc比较-数据库vs应用">2PC比较：数据库VS应用&lt;/h2>
&lt;p>比较容易取得共识的结论：不同业务系统之间使用2PC。&lt;/p>
&lt;p>那剩下的问题就简单了, 相同业务系统之间是使用数据访问层2PC还是TCC？一般而言，基于研发成本考虑，会建议：新系统由数据库层来实现统一的分布式事务。&lt;/p>
&lt;p>但对热点数据，例如商品（票券等）库存，建议使用TCC方案，因为TCC的主要优势正是可以避免长时间锁定数据库资源进而提高并发性。&lt;/p>
&lt;p>2PC、3PC、TCC都是基于XA协议思想，TCC是应用层/业务层，TCC实际上为2PC的变种。&lt;/p>
&lt;h2 id="2pc与paxos">2PC与Paxos&lt;/h2>
&lt;p>有一种广为流传的观点:&amp;quot;2PC到3PC到Paxos到Raft&amp;quot;，即认为:&lt;/p>
&lt;p>• 2PC是Paxos的残次版本&lt;/p>
&lt;p>• 3PC是2PC的改进&lt;/p>
&lt;p>上述2个观点都是我所不认同的，更倾向于如下认知：&lt;/p>
&lt;p>• 2PC与Paxos解决的问题不同：2PC是用于解决数据分片后，不同数据之间的分布式事务问题；而Paxos是解决相同数据多副本下的数据一致性问题。例如，UP-2PC的数据存储节点可以使用MGR来管理统一数据分片的高可用&lt;/p>
&lt;p>• 3PC只是2PC的一个实践方法：一方面并没有完整解决事务管理器宕机和资源管理器宕机等异常，反而因为增加了一个处理阶段让问题更加复杂&lt;/p>
&lt;h1 id="paxos算法">Paxos算法&lt;/h1>
&lt;p>读音：/pakses/&lt;/p>
&lt;p>Paxos算法是LeslieLamport1990年提出的一种一致性算法, 该算法是一种提高分布式系统容错性的一致性算法, 解决了3PC中网络分区的问题, paxos算法可以在节点失效、网络分区、网络延迟等各种异常情况下保证所有节点都处于同一状态, 同时paxos算法引入了“过半“理念, 即少数服从多数原则。&lt;/p>
&lt;p>&lt;strong>paxos有三个版本:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>BasicPaxos&lt;/li>
&lt;li>MultiPaxos&lt;/li>
&lt;li>FastPaxos&lt;/li>
&lt;/ul>
&lt;h2 id="四种角色">四种角色&lt;/h2>
&lt;p>在paxos算法中, 有四种种角色, 分别具有三种不同的行为, 但多数情况, 一个进程可能同时充当多种角色。&lt;/p>
&lt;ul>
&lt;li>client: 系统外部角色, 请求发起者, 不参与决策&lt;/li>
&lt;li>proposer: 提案提议者&lt;/li>
&lt;li>acceptor: 提案的表决者, 即是否accept该提案, 只有超过半数以上的acceptor接受了提案, 该提案才被认为被“选定“&lt;/li>
&lt;li>learners: 提案的学习者, 当提案被选定后, 其同步执行提案, 不参与决策&lt;/li>
&lt;/ul>
&lt;h2 id="两个阶段">两个阶段&lt;/h2>
&lt;p>prepare阶段（准备解决）、accept阶段（同意阶段）&lt;/p>
&lt;p>1、prepare阶段&lt;/p>
&lt;p>&amp;lt;1&amp;gt; proposer提出一个提案, 编号为N, 发送给所有的acceptor。&lt;/p>
&lt;p>&amp;lt;2&amp;gt; 每个表决者都保存自己的accept的最大提案编号maxN, 当表决者收到prepare(N) 请求时, 会比较N与maxN的值, 若N小于maxN, 则提案已过时, 拒绝prepare(N) 请求。若N大于等于maxN, 则接受提案, 并将该表决者曾经接受过的编号最大的提案Proposal(myid, maxN, wvalue) 反馈给提议者: 其中myid表示表决者acceptor的标识id, maxN表示接受过的最大提案编号maxN, value表示提案内容。若当前表决者未曾accept任何提议, 会将proposal(myid, nullnull) 反馈给提议者。&lt;/p>
&lt;p>2、accept阶段&lt;/p>
&lt;p>&amp;lt;1&amp;gt; 提议者proposa|发出prepare(N) , 若收到超过半数表决者acceptor的反馈, proposal将真正的提案内容proposal(N, wvalue) 发送给所有表决者。&lt;/p>
&lt;p>&amp;lt;2&amp;gt; 表决者aCCeptor接受提议者发送的proposal提案后, 会将自己曾经accept过的最大&lt;/p>
&lt;p>提案编号maxN和反馈过的prepare的最大编号, 若N大于这两个编号, 则当前表决者accept该提&lt;/p>
&lt;p>案, 并反馈给提议者。否则拒绝该提议。&lt;/p>
&lt;p>&amp;lt;3&amp;gt; 若提议者没有收到半数以上的表决者accept反馈, 则重新进入prepare阶段, 递增提案编号, &lt;/p>
&lt;p>重新提出prepare请求。若收到半数以上的accept, 则其他未向提议者反馈的表决者称为&lt;/p>
&lt;p>learner, 主动同步提议者的提案。&lt;/p>
&lt;p>正常流程&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_consistency_211219_4.png" alt="distributed_consistency_211219_4.png">&lt;/p>
&lt;p>单点故障，部分节点失败&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_consistency_211219_5.png" alt="distributed_consistency_211219_5.png">&lt;/p>
&lt;p>proposer失败&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_consistency_211219_6.png" alt="distributed_consistency_211219_6.png">&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_consistency_211219_7.png" alt="distributed_consistency_211219_7.png">&lt;/p>
&lt;p>Basic Paxos算法存在活锁问题（liveness）或全序问题&lt;/p>
&lt;blockquote>
&lt;p>活锁：并发导致的你让我我让你，如对面走路互相让路还是会冲突。避免活锁：执行时间错开。
注意Basic Paxos的活锁原因是proposer1在发送请求给accepter之后挂掉了，然后proposer2又向accepter发送了新的请求，然后导致的活锁问题。更深层proposer可以有多个，所以mutil paxos采用了leader（单个）替代proposer（多个）解决活锁问题，如果leader挂掉就重新选举，都是leader说了算就不会产生活锁问题了。&lt;/p>
&lt;/blockquote>
&lt;h2 id="消化理解">消化理解&lt;/h2>
&lt;h3 id="paxos算法-1">paxos算法&lt;/h3>
&lt;p>基于消息传递一关有高度容错性的一种算法, 是目前公认的解决分布式一致性问题最有效的算法&lt;/p>
&lt;h3 id="重要概念">重要概念&lt;/h3>
&lt;p>半数原则: 少数服从多数&lt;/p>
&lt;h3 id="解决问题">解决问题&lt;/h3>
&lt;p>在分布式系统中, 如果产生容机或者网络异常情况, 快速的正确的在集群内部对染个数据的信达成一致, 并玟不管发生任何异常, 都不会破坏整个系统的一致性&lt;/p>
&lt;h4 id="复杂度问题">复杂度问题&lt;/h4>
&lt;p>为了解决活锁问题，出现了multi-paxos；&lt;/p>
&lt;p>为了解决通信次数较多的问题，出现了fast-paxos；&lt;/p>
&lt;p>为了尽量减少冲突，出现了epaxos。&lt;/p>
&lt;p>可以看到，工业级实现需要考虑更多的方面，诸如性能，异常等等。这也是许多分布式的一致性框架并非真正基于paxos来实现的原因。&lt;/p>
&lt;h4 id="全序问题">全序问题&lt;/h4>
&lt;p>对于paxos算法来说，不能保证两次提交最终的顺序，而zookeeper需要做到这点。&lt;/p>
&lt;h3 id="理解basic-paxos细节">理解basic paxos细节&lt;/h3>
&lt;ul>
&lt;li>proposer是可以有多个，所以他叫做proposer，而不叫leader&lt;/li>
&lt;li>proposer并不强制要发送propose到全部acceptor，也可以发送70%的acceptor，只要通过的有半数以上，就认为协议是成功的&lt;/li>
&lt;li>容易看出，第二阶段的时候client仍需要等着，只有在第二阶段大半acceptor返回accepted后，client才能得到成功的信息&lt;/li>
&lt;li>有一些错误场景中，proposer会互相锁住对方的递交，详细可以看wiki，这里不多阐述：&lt;a href="https://en.wikipedia.org/wiki/Paxos_(computer_science)">Paxos (computer science)&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="multi-paxos">Multi Paxos&lt;/h2>
&lt;h3 id="改进点">改进点&lt;/h3>
&lt;ul>
&lt;li>Multi Paxos中采用了 Leader election 保证了只有一个Proposer，避免了活锁的问题&lt;/li>
&lt;li>Basic Paxos中只有Proposer知道 选择了哪个value，如果其他server想知道选择了哪个，那么就得按照paxos协议发起请求。&lt;/li>
&lt;li>multi paxos 目标是实现复制日志序列；实现的时候我们在Prepare和accept的时候，加上日志记录的序号即可。&lt;/li>
&lt;/ul>
&lt;h2 id="fast-paxos">&lt;strong>Fast Paxos&lt;/strong>&lt;/h2>
&lt;p>在Multi Paxos中，&lt;strong>proposer -&amp;gt; leader -&amp;gt; acceptor -&amp;gt; learner&lt;/strong>，从提议到完成决议共经过3次通信，能不能减少通信步骤？&lt;/p>
&lt;p>对Multi Paxos phase2a，如果可以自由提议value，则可以让proposer直接发起提议、leader退出通信过程，变为&lt;strong>proposer -&amp;gt; acceptor -&amp;gt; learner&lt;/strong>，这就是Fast Paxos[2]的由来。&lt;/p>
&lt;p>Multi Paxos里提议都由leader提出，因而不存在一次决议出现多个value，Fast Paxos里由proposer直接提议，一次决议里可能有多个proposer提议、出现多个value，即出现提议冲突(collision)。&lt;strong>leader起到初始化决议进程(progress)和解决冲突的作用，当冲突发生时leader重新参与决议过程、回退到3次通信步骤。&lt;/strong>&lt;/p>
&lt;p>Paxos自身隐含的一个特性也可以达到减少通信步骤的目标，如果acceptor上一次确定(chosen)的提议来自proposerA，则当次决议proposerA可以直接提议减少一次通信步骤。如果想实现这样的效果，需要在proposer、acceptor记录上一次决议确定(chosen)的历史，用以在提议前知道哪个proposer的提议上一次被确定、当次决议能不能节省一次通信步骤。&lt;/p>
&lt;h2 id="epaxos">&lt;strong>EPaxos&lt;/strong>&lt;/h2>
&lt;p>除了从减少通信步骤的角度提高Paxos决议效率外，还有其他方面可以降低Paxos决议时延，比如Generalized Paxos[3]提出不冲突的提议(例如对不同key的写请求)可以同时决议、以降低Paxos时延。&lt;/p>
&lt;p>更进一步地，EPaxos[4](Egalitarian Paxos)提出一种既支持不冲突提议同时提交降低时延、还均衡各节点负载、同时将通信步骤减少到最少的Paxos优化方法。&lt;/p>
&lt;p>为达到这些目标，EPaxos的实现有几个要点：&lt;/p>
&lt;ul>
&lt;li>一是EPaxos中没有全局的leader，而是每一次提议发起提议的proposer作为当次提议的leader(command leader)；&lt;/li>
&lt;li>二是不相互影响(interfere)的提议可以同时提交；&lt;/li>
&lt;li>三是跳过prepare，直接进入accept阶段。&lt;/li>
&lt;/ul>
&lt;h1 id="zab协议">ZAB协议&lt;/h1>
&lt;p>由于paxos算法窍现起来较难, 存在活锁和全序问题(无法保证两次最终提交的顺序) , 所以zookeeper并没有使用paxos作为一致性协议, 而是使用了ZAB协议。&lt;/p>
&lt;p>ZAB（zookeeper atomic broadcast) : 是一种支持崖溃恢复的原子广播协议, 基于&lt;strong>multi&lt;/strong> &lt;strong>paxos&lt;/strong>实现&lt;/p>
&lt;p>ZooKeeper使用&lt;strong>单一主进程Leader用于处理客户端所有事务请求, , 即写请求&lt;/strong>。当服务器数据发生变更好, 集群采用ZAB原子广播协议, 以事务提交proposal的形式广播到所有的副本进程, 每一个事务分配一个全局的递增的事务编号xid。&lt;/p>
&lt;p>&lt;strong>若客户端提交的请求为读请求时, 则接受请求的节点直接根据自己保存的数据响应&lt;/strong>。**若是写请求, 且当前节点不是leader, 那么该节点就会将请求转发给leader, leader会以提案的方式广播此写请求, 如果超过半数的节点同意写请求, 则该写请求就会提交。**leader会通知所有的订阅者同步数据。&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_consistency_211219_8.png" alt="distributed_consistency_211219_8.png">&lt;/p>
&lt;h2 id="三种角色">三种角色&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>leader
负责处理集群的写请求，并发起投票，只有超过半数的节点同意后才会提交该写请求&lt;/p>
&lt;/li>
&lt;li>
&lt;p>follower
处理读请求，响应结果。转发写请求到leader，在选举leader过程中参与投票&lt;/p>
&lt;/li>
&lt;li>
&lt;p>observer
可以理解为没有投票权的follower，主要职责是协助follower处理读请求。那么当整个zk集群读请求负载很高时（此时会使用observer缓解读请求压力），为什么不直接增加follow节点而是增加observer节点呢？原因是增加follower节点会让leader在提出写请求提案时，需要半数以上的follower投票节点同意，这样会增加leader和foloower的通信压力，降低写操作效率&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="两种模式">两种模式&lt;/h2>
&lt;h3 id="恢复模式">恢复模式&lt;/h3>
&lt;p>当服务启动或领导崩溃后, zk进入恢复状态, 选举leader, leader选出后, 将完成leader别其他机器的数据同步, 当大多数server完成和leader的同步后, 恢复模式结束&lt;/p>
&lt;h3 id="广播模式">广播模式&lt;/h3>
&lt;p>&lt;strong>一旦Leader已经和多数的Follower进行了状态同步后, 进入广播模式&lt;/strong>。进入广播模式后, 如果有&lt;/p>
&lt;p>新加入的服务器, 会自动从leader中同步数据。leader在接收客户端请求后, 会生成事务提案广播&lt;/p>
&lt;p>给其他机器, 有超过半数以上的follower同意该提议后, 再提交事务。&lt;/p>
&lt;p>&lt;strong>注意在ZAB的事务的二阶段提交中, 移除了事务中断的逻辑, follower要么ack, 要么放弃, leader无需等待所有的follower的ack。&lt;/strong>&lt;/p>
&lt;h2 id="zxid">zxid&lt;/h2>
&lt;p>zxid是64位长度的Long类型, 其中高32位表示纪元epoch, 低32位表示事务标识xid。即zxid由两部分构成: epoch和xid&lt;/p>
&lt;p>每个leader都会具有不同的epoch值, 表示一个纪元, 每一个新的选举开启时都会生成一个新的&lt;/p>
&lt;p>epoch, 新的leader产生, 会更新所有的zKServer的zxid的epoch, xid是一个依次递增的事务编号。&lt;/p>
&lt;h2 id="leader选举算法">leader选举算法&lt;/h2>
&lt;h3 id="三个核心选举原则">三个核心选举原则&lt;/h3>
&lt;p>1、集群中只有超过了半数以上的服务器启动，集群才能正常工作&lt;/p>
&lt;p>2、在集群正常工作之前，myid小的服务器会给myid大的服务器进行，持续到集群正常工作，选出leader&lt;/p>
&lt;p>3、选择leader之后，之前的服务器状态由looking改变为following，以后的服务器都是follower&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_consistency_211219_9.png" alt="distributed_consistency_211219_9.png">&lt;/p>
&lt;h3 id="启动过程">启动过程&lt;/h3>
&lt;ul>
&lt;li>每一个server发出一个投票给集群中其他节点&lt;/li>
&lt;li>收到各个服务器的投票后, 判断该投票有效性, 比如是否是本轮投票（解决活锁问题）, 是否是looking状态&lt;/li>
&lt;li>处理投票, pk别人的投票和自己的投票比较规则xid &amp;gt; myid“取大原则“&lt;/li>
&lt;li>统计是否超过半数的接受相同的选票&lt;/li>
&lt;li>确认leader, 改变服务器状态&lt;/li>
&lt;li>添加新server, leader已经选举出来, 只能以follower身份加入集群中&lt;/li>
&lt;/ul>
&lt;h3 id="崩溃恢复过程">崩溃恢复过程&lt;/h3>
&lt;ul>
&lt;li>leader挂掉后, 集群中其他follower会将状态从FOLLOWING变为LOOKING, 重新进入leader选举&lt;/li>
&lt;li>同上启动过程&lt;/li>
&lt;/ul>
&lt;h3 id="消息广播算法">消息广播算法&lt;/h3>
&lt;p>一旦进入广播模式, 集群中非leader节点接受到事务请求, 首先会将事务请求转发给服务器, leader服务器为其生成对应的事务提案proposal，并发送给集群中其他节点，如果过半则事务提交；&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_consistency_211219_10.png" alt="distributed_consistency_211219_10.png">&lt;/p>
&lt;ul>
&lt;li>leader接收到消息后, 消息通过全局唯一的64位自增事务id，zxid标识&lt;/li>
&lt;li>leader发送给follower的提案是有序的, leader会创建一个FIFFO队列, 将提案顺序写入队列中发送给follower&lt;/li>
&lt;li>follower接受到提案后, 会比较提案zxid和本地事务日志最大的zxid, 若提案zxid比本地事务id大（保证全序）, 将提案记录到本地日志中, 反馈ack给leader, 否则拒绝&lt;/li>
&lt;li>leader接收到过半ack后, leader向所有的follower发送commit, 通知每个follower执行本地事务&lt;/li>
&lt;/ul>
&lt;h2 id="zab与paxos比较">ZAB与Paxos比较&lt;/h2>
&lt;p>ZAB和Paxos最大的不同是，&lt;strong>ZAB主要是为分布式主备系统设计的，而Paxos的实现是一致性状态机(state machine replication)&lt;/strong>&lt;/p>
&lt;p>尽管ZAB不是Paxos的实现，但是ZAB也参考了一些Paxos的一些设计思想，比如：&lt;/p>
&lt;ul>
&lt;li>leader向follows提出提案(proposal)&lt;/li>
&lt;li>leader 需要在达到法定数量(半数以上)的follows确认之后才会进行commit&lt;/li>
&lt;li>每一个proposal都有一个纪元(epoch)号，类似于Paxos中的选票(ballot)&lt;/li>
&lt;/ul>
&lt;h1 id="reference">Reference&lt;/h1>
&lt;p>&lt;a href="https://www.cnblogs.com/raphael5200/p/5285583.html">zookeeper工作原理&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://b23.tv/UpL9jf">Paxos基本入门&lt;/a>&lt;/p></description></item><item><title>Docs: 分布式</title><link>/docs/41.%E5%88%86%E5%B8%83%E5%BC%8F/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/41.%E5%88%86%E5%B8%83%E5%BC%8F/</guid><description>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>分布式&lt;/p></description></item><item><title>Docs: 常见分布式拓扑结构</title><link>/docs/41.%E5%88%86%E5%B8%83%E5%BC%8F/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83%E5%BC%8F%E6%8B%93%E6%89%91%E7%BB%93%E6%9E%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/docs/41.%E5%88%86%E5%B8%83%E5%BC%8F/%E5%B8%B8%E8%A7%81%E5%88%86%E5%B8%83%E5%BC%8F%E6%8B%93%E6%89%91%E7%BB%93%E6%9E%84/</guid><description>
&lt;h1 id="背景">背景&lt;/h1>
&lt;p>分布式架构一般有以下几种结构：&lt;/p>
&lt;ul>
&lt;li>主从架构&lt;/li>
&lt;li>去中心化架构&lt;/li>
&lt;li>混合型架构&lt;/li>
&lt;/ul>
&lt;h1 id="redis分布式模式">Redis分布式模式&lt;/h1>
&lt;p>Redis一般有两种模式：&lt;/p>
&lt;ul>
&lt;li>主从模式&lt;/li>
&lt;li>集群模式（Redis Cluster）&lt;/li>
&lt;/ul>
&lt;h2 id="主从模式">主从模式&lt;/h2>
&lt;h3 id="标准主从模式">标准主从模式&lt;/h3>
&lt;p>标准的Redis主从模式，只有 Master 节点接收写入请求，并将写入的数据复制给一个或多个Slave，形成良好的读写分离机制，多个 Slave 可以分担读请求。Redis 主从是标准的分布式中心化思想。&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_architecture_1.png" alt="distributed_architecture_1.png">&lt;/p>
&lt;h3 id="树形主从模式">树形主从模式&lt;/h3>
&lt;p>Redis 应用场景大多是极高并发的内存 I/O，常见应用场景是作为数据库的防护网，防止数据库被击穿，因此标准的主从模式中的 Master 对外既要承担写入，对内又要承担各个节点的复制操作，资源消耗很大，且随着 slave 节点增多问题越发明显。因此又形成主从的一个变形形式。&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_architecture_2.png" alt="distributed_architecture_2.png">&lt;/p>
&lt;h2 id="集群模式">集群模式&lt;/h2>
&lt;p>对于高并发的业务场景，Master 始终是一个隐患。因为 Master 承担着所有的写操作，如果没有HA解决方案一旦宕机，集群将不可用。因此 Redis 社区推出集群方案，主要是为了解决 Master 压力，即集群的分布式无中心模式。&lt;/p>
&lt;p>&lt;img src="../imgs/distributed_architecture_3.png" alt="distributed_architecture_3.png">&lt;/p>
&lt;p>无中心模式采用虚拟槽概念，独立于物理节点。虚拟槽有 0～16383 个，数据会进行 key 的hash计算，确定进入哪个槽位。而物理节点负责哪些虚拟槽，即对应关系可以自行指定。&lt;/p>
&lt;p>每个节点（数据节点）都是&lt;strong>对等节点&lt;/strong>。此外，为了高可靠HA，每个节点也可以再演变成 Master 和 Slave 的主从模式部署。&lt;/p>
&lt;h2 id="小结">小结&lt;/h2>
&lt;p>Redis 最成熟的方案还是主从模式，Redis Cluster 带来的性能优势无法抵消去中心化带来的不成熟和不可靠问题，导致人工运维的复杂度和难度。生产中慎用 Redis Cluster。&lt;/p>
&lt;h1 id="kafka">Kafka&lt;/h1>
&lt;h2 id="架构">架构&lt;/h2>
&lt;p>&lt;img src="../imgs/distributed_architecture_4.png" alt="distributed_architecture_4.png">&lt;/p>
&lt;p>Kafka 集群使用 Zookeeper 集群来管理，Broker 的注册发现等都是依靠 zookeeper 来协助完成。每个 Broker 会以一主二从（Leader 和 Follower）的方式均匀分布。&lt;/p>
&lt;p>生产者（Product）从任意节点获取 Meta 信息，找到 Broker 中的 Leader 分区副本，会向里面写分配好的数据，Leader 会向集群中其他 Broker 的 Follower 分区副本复制。&lt;/p>
&lt;h2 id="小结-1">小结&lt;/h2>
&lt;p>Broker 分区信息是分布在每一台 Broker 的 meta 缓存里面，生产者和消费者可以在任意一台 Borker 上获取需要操作的 Leader 分区信息，Kafka 这种设计有点去中心化的意思。但是这些 meta 缓存信息实质是来自 Zookeeper（强依赖），所以本质上 Kafka 依然是中心化管理。&lt;/p>
&lt;h1 id="rocketmq">RocketMQ&lt;/h1>
&lt;h2 id="架构-1">架构&lt;/h2>
&lt;p>&lt;img src="../imgs/distributed_architecture_5.png" alt="distributed_architecture_5.png">&lt;/p>
&lt;p>RocketMQ 总体结构与 Kafka 类似，更适合高并发场景，使用 NameServer 替代了 Zookeeper。&lt;/p>
&lt;p>NameServer 是无中心节点的，都通过锁注册表的方式共享信息。NameServer 是 Broker 的注册表，Broker 新注册或者异常退出，对应的 NameSever 都会感知到。NameServer 增加/删除所辖 Broker 信息到该注册表，并定时读取最新的集群所有broker信息。&lt;/p>
&lt;p>生产者（Producet）连接一个 NameServer，便能获取到想要发送分区数据的 Brokers（消费者同理）。&lt;/p>
&lt;p>每个 Borker 可以再分成主从模式，Master 进行队列操作，Slave 只做数据同步，Master 出现故障时进行替换。&lt;/p>
&lt;h2 id="小结-2">小结&lt;/h2>
&lt;p>NameSever 相对于 Zookeeper 的结构更简单，生产者和消费者对 Broker 以及分区的获取必须来自 NameSever，尽管 NameSever 集群本身是无中心的，但整个 RocketMQ 的 Brokers 是被 NameSever 中心化管理的，但整体上 Product、Consumer、Brokers集群对这种集中管理的依赖程度并不高，只是提供了很简单的 Broker 元信息服务，真正的数据流还是交给各个Broker 自行解决。&lt;/p></description></item></channel></rss>