<!doctype html><html lang=zh-cn class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.101.0"><link rel=canonical type=text/html href=/docs/52.%E5%A4%A7%E6%95%B0%E6%8D%AE/><link rel=alternate type=application/rss+xml href=/docs/52.%E5%A4%A7%E6%95%B0%E6%8D%AE/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>大数据 | Herbdocs</title><meta name=description content="Introduction
大数据相关，如Hadoop/Spark/HDFS/HIVE/HBASE/Flink等
"><meta property="og:title" content="大数据"><meta property="og:description" content="Herb's blog"><meta property="og:type" content="website"><meta property="og:url" content="/docs/52.%E5%A4%A7%E6%95%B0%E6%8D%AE/"><meta property="og:site_name" content="Herbdocs"><meta itemprop=name content="大数据"><meta itemprop=description content="Herb's blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="大数据"><meta name=twitter:description content="Herb's blog"><link rel=preload href=/scss/main.min.dcab2c3dae6e6fefb7672f19e4b612cc4e75ca208740f642aa2f1fe378478a8f.css as=style><link href=/scss/main.min.dcab2c3dae6e6fefb7672f19e4b612cc4e75ca208740f642aa2f1fe378478a8f.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-00000000-0","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><img width=6% style=margin-bottom:1% src=/favicons/favicon.ico></span><span class=font-weight-bold>&nbsp;Herbdocs</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/about/><span>About</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/docs/><span class=active>Documentation</span></a></li><li class="nav-item dropdown mr-4 d-none d-lg-block"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Chinese</a><div class=dropdown-menu aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/en/>English</a></div></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; 站内搜索…" aria-label=站内搜索… autocomplete=off></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.</p><p><a href=/docs/52.%E5%A4%A7%E6%95%B0%E6%8D%AE/>返回本页常规视图</a>.</p></div><h1 class=title>大数据</h1><ul><li>1: <a href=#pg-139b9cd868115e59811f01333853c62a>初探Hbase</a></li><li>2: <a href=#pg-f11be7a479d8a489a28499603c436a50>大数据框架</a></li><li>3: <a href=#pg-54b122ccb8b94319f23c1def3e2ff60c>HDFS-01基本介绍</a></li><li>4: <a href=#pg-13f4b5c2b3adc66b901f3693d67aab0f>HDFS-02基本命令</a></li><li>5: <a href=#pg-c937e020882673d7b1c7e8747009259e>Spark</a></li><ul><li>5.1: <a href=#pg-003faf893f62ece2e3e51f9693a847bd>01.Spark基本介绍</a></li></ul></ul><div class=content><h1 id=introduction>Introduction</h1><p>大数据相关，如Hadoop/Spark/HDFS/HIVE/HBASE/Flink等</p></div></div><div class=td-content><h1 id=pg-139b9cd868115e59811f01333853c62a>1 - 初探Hbase</h1><h1 id=hbase是什么>HBase是什么</h1><p>HBase是一种构建在HDFS之上的分布式键值存储系统。</p><p>HBase 是Google Bigtable 的开源实现。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。</p><p>另一个不同的是HBase基于列的而不是基于行的模式。</p><p><strong>适用场景</strong></p><ul><li>存储大量数据(PB级数据)。</li><li>高并发写入，瞬间写入量很大（写多，读少）。</li><li>业务场景简单(无jion，事务), 按单一维度查询（基于rowkey）。</li><li>非结构化的数据存储，列可以优雅扩展。</li></ul><p><strong>不适用场景</strong></p><ul><li>事务。</li><li>join、group by。</li><li>除了rowkey之外的复杂查询。</li><li>高并发，低延迟随机读 。</li></ul><p><strong>MT应用场景</strong></p><ul><li>MTtrace</li><li>云搜</li></ul><h1 id=行存储vs列存储><strong>行存储VS列存储</strong></h1><p><img src=../imgs/hbase_basic_211212_1.png alt=hbase_basic_211212_1.png></p><p>**行式存储：**一张表的数据都是放在一起</p><p>**列式存储：**以列为单位聚合数据，不同的列分开存储</p><table><thead><tr><th style=text-align:left></th><th style=text-align:left>行式存储</th><th style=text-align:left>列式存储</th></tr></thead><tbody><tr><td style=text-align:left><strong>优点</strong></td><td style=text-align:left>一行记录的写入是一次完成,消耗的时间比列存储少。</td><td style=text-align:left>读取过程，按列读取不会产生冗余数据。适合列很多，但每次只需查询少数列的场景。</td></tr><tr><td style=text-align:left><strong>缺点</strong></td><td style=text-align:left>数据读取时，将一行数据完全读出。存在冗余列</td><td style=text-align:left>需要将一行记录拆分成单列保存，写入次数更多，时间消耗会更大。</td></tr></tbody></table><h1 id=hbase数据模型><strong>HBase数据模型</strong></h1><p>HBase 以列族为区分列式存储数据库。表可以被看成是一个稀疏的行的集合。一个列族是多个column的集合.</p><p><img src=../imgs/hbase_basic_211212_2.png alt=hbase_basic_211212_2.png></p><p><strong>物理视图</strong></p><p><img src=../imgs/hbase_basic_211212_3.png alt=hbase_basic_211212_3.png></p><ul><li>anchor 、contents 分别为两个列族，区分存储。</li><li>cnnsi.com 、my.look.ca为列族anchor两个列，html为列族contents的列。</li></ul><p><strong>行键</strong></p><p>Row Key 是用来检索记录的主键。Row Key 可定义任意字符串，如订单id,事务id。</p><p>在HBase 内部，Row Key 保存为字节数组。HBase表的行是按Row Key字典序存储的。</p><p><strong>列族</strong></p><p>列族一些列的集合，列族必须在表建立的时候声明。column就不需要了，随时可以新建。</p><p>在物理上，一个的列族成员在文件系统上都是存储在一起。因为存储优化都是针对列族级别的。</p><p>这就意味着，在表设计的时候要考虑一个colimn family的所有成员的是否有相同的访问方式。</p><p><strong>Cells和版本</strong></p><p>对于同一Row Key 的相同列的多次写操作，使用版本来区分。可以把版本理解每次写入的快照。</p><p>A *{row, column, version}*元组就是一个HBase中的一个单元。Cell的内容是不可分割的字节数组，即我们存储和具体的值。</p><p>可保存的版本数需要设定，读取这个文件的时候，默认是最近的版本。</p><p><strong>操作</strong></p><p>主要操作有</p><p>Get：指定Row Key查找，性能最高。</p><p>Scan ：基于Row Key 前缀查找，或全表扫描。</p><p>Put：向表增加新行 (如果key是新的) 或更新行 (如果key已经存在)。</p><p>Bulk Loading：批量装载，批量装载特性采用 MapReduce 任务，将表数据输出为HBase的内部数据格式，然后可以将产生的存储文件直接装载到运行的集群中。</p><p>Delete：从表中删除一行。</p><p><strong>TTL</strong></p><p>存活时间——列族可以设置TTL秒数，HBase 在超时后将自动删除数据</p><h1 id=hbase表设计><strong>HBase表设计</strong></h1><p><strong>rowkey</strong></p><p>HBase 的检索都是基于 rowkey，类似sql 里的like 操作，我们需要根据查询场景来合理设置rowkey。参考<a href=https://km.sankuai.com/page/59718515>合理设计hbase rowkey</a></p><p>1：查询最左匹配原则</p><p>假设查询包含3个维度：shopId，orderId ，如果将rowkey 定义为: shopId_orderId</p><p>则以下维度的查询比较高效</p><p>(1) 通过shopId查询</p><p>(2) 通过shopId + orderId查询</p><p>但通过orderId查询则比较低效，为全表扫描操作</p><p>2：避免热点Region</p><p>HBase 的行会根据rowkey 拆分到不同的 Region 中。</p><p>如果是连续自增性质的rowkey，则相邻rowkey写入到了同一个Region里，产生热点Region。热点Region容易导致读写出现性能瓶颈。</p><p>一般的做法是在rowkey 加一个hash前缀。比如hash(shopId)_shopId_orderId</p><p>3：避免短键过长</p><p>在满足需求的情况下，行键越短越好。</p><p><strong>列簇</strong></p><p>建议列族不要超过3个，按照访问维度划分。</p><p>尽量使列族名小，最好一个字符。(如 "d" 表示 data/default)。</p><p><strong>列名</strong></p><p>最好还是用短属性名，节约存储空间。</p><p><strong>版本数</strong></p><p>每个列族可以单独设置，默认是3。按业务需求要合理设置。</p><p><strong>数据类型</strong></p><p>任何可被转为字节数组的东西可以作为值存入，输入可以是字符串，数字，复杂对象，甚至图像，只要他们能转为字节。</p><p><strong>demo</strong></p><p>业务场景：评价审核日志收集。</p><p>从新增（编辑）一条评价到 诚信审核反馈并储存审核结果完成，定义为一个<strong>送审事务。</strong></p><p>每个送审事务都有一个<strong>唯一标识</strong>（transId），在整个事务的各环节包括 :**评价信息存储->评价送审->****诚信审核->审核反馈->**<strong>审核结果存储</strong>。</p><p>针对每个送审事务的各环节，进行日志收集。</p><p>查询场景1：根据评价id ，查询所有的审核日志。</p><p>查询场景2：根据评价id 和事务id, 查询该事务的审核日志。</p><p>表结构定义：</p><table><thead><tr><th style=text-align:left>参数</th><th style=text-align:left></th></tr></thead><tbody><tr><td style=text-align:left>表名</td><td style=text-align:left>〜〜〜〜</td></tr><tr><td style=text-align:left>TTL</td><td style=text-align:left>永久</td></tr><tr><td style=text-align:left>Row Key</td><td style=text-align:left>hash(BizType_BizID)_BizType_BizID_AuditTransID</td></tr><tr><td style=text-align:left>版本数</td><td style=text-align:left>3（针对同一个审核事务，诚信会有多次反馈结果，可保留多个版本的反馈结果）</td></tr><tr><td style=text-align:left>列族</td><td style=text-align:left>b:基础信息族<br>e:扩展信息族</td></tr></tbody></table><p><strong>Columns</strong></p><table><thead><tr><th style=text-align:left>列族</th><th style=text-align:left>字段名</th><th style=text-align:left>字段名缩写</th><th style=text-align:left><strong>类型</strong></th><th style=text-align:left><strong>含义</strong></th><th style=text-align:left><strong>备注</strong></th></tr></thead><tbody><tr><td style=text-align:left>基础信息族（b）</td><td style=text-align:left>TransType</td><td style=text-align:left>ty</td><td style=text-align:left>tinyint(4)</td><td style=text-align:left>事务类型</td><td style=text-align:left>0：用户发起 1：诚信回扫 2：用户申诉 3：用户举报 4:ugc 发起</td></tr><tr><td style=text-align:left></td><td style=text-align:left>Version</td><td style=text-align:left>vs</td><td style=text-align:left>bigint(20)</td><td style=text-align:left>送审版本</td><td style=text-align:left>内容生成时间戳</td></tr><tr><td style=text-align:left></td><td style=text-align:left>EventType0</td><td style=text-align:left>et0</td><td style=text-align:left>bigint(20)</td><td style=text-align:left>事件0发生时间</td><td style=text-align:left>新增完成事件</td></tr><tr><td style=text-align:left></td><td style=text-align:left>EventType1</td><td style=text-align:left>et1</td><td style=text-align:left>bigint(20)</td><td style=text-align:left>事件1发生时间</td><td style=text-align:left>诚信送审完成事件</td></tr><tr><td style=text-align:left></td><td style=text-align:left>EventType2</td><td style=text-align:left>et2</td><td style=text-align:left>bigint(20)</td><td style=text-align:left>事件2发生时间</td><td style=text-align:left>诚信审核反馈事件</td></tr><tr><td style=text-align:left></td><td style=text-align:left>....</td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td></tr><tr><td style=text-align:left></td><td style=text-align:left><strong>AuditResult</strong></td><td style=text-align:left></td><td style=text-align:left><strong>tinyint(4)</strong></td><td style=text-align:left>诚信审核结果</td><td style=text-align:left><strong>0:无 1:通过 2: 违规</strong><br><strong>AuditResultEnum</strong></td></tr><tr><td style=text-align:left></td><td style=text-align:left><strong>AuditDetail</strong></td><td style=text-align:left></td><td style=text-align:left>string</td><td style=text-align:left>诚信审核标签明细</td><td style=text-align:left>{处理建议，多标签...}</td></tr><tr><td style=text-align:left></td><td style=text-align:left><strong>AuditTime</strong></td><td style=text-align:left></td><td style=text-align:left>bigint(20)</td><td style=text-align:left>诚信审核时间</td><td style=text-align:left></td></tr><tr><td style=text-align:left></td><td style=text-align:left>....</td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td><td style=text-align:left></td></tr></tbody></table><h1 id=hbase系统架构><strong>HBase系统架构</strong></h1><p><img src=../imgs/hbase_basic_211212_4.png alt=hbase_basic_211212_4.png></p><p><strong>Client</strong></p><p>包含访问HBase的接口，并维护cache来加快对HBase的访问。</p><p>对于管理类操作，Client与HMaster进行RPC。</p><p>对于数据读写操作，Client与HRegionServer进行RPC。</p><p><strong>Zookeeper</strong></p><p>保证任何时候，集群中只有一个master。</p><p>实时监控Region server的上线和下线信息。并实时通知给master。</p><p>存储HBase的schema和table元数据。</p><p><strong>Master</strong></p><p>为Region server分配region，负责Region server的负载均衡.</p><p>发现失效的Region server并重新分配其上的region。</p><p>处理表的建立，删除等操作。</p><p><strong>Region Server</strong></p><p>维护master分配给他的region，处理对这些region的io请求。</p><p>负责切分正在运行过程中变的过大的region。</p><p>当用户需要对数据进行读写操作时，需要访问HRegionServer。</p><p><strong>Region</strong></p><p>table在行的方向上分隔为多个Region。</p><p>随着数据不断插入表，当region的某个列族达到一个阈值时就会拆分新的region。</p><p><strong>Store</strong></p><p>每一个region由一个或多个store组成，hbase会为每个列族建一个store。</p><p>HStore存储由两部分组成：MemStore和StoreFiles。 写入数据首先会放在MemStore,当MemStore满了以后会Flush成一个 StoreFile（实际存储在HDHS上的是HFile）。</p><p>写操作只要进入内存中就可以立即返回，保证了HBase I/O的高性能。</p><p><strong>HFile</strong></p><p>HFile就是实际的存储文件。HFile由多个Data Block组成，Data Block是HBase的最小存储单元。</p><p>HBase 会基于Data Block的做缓存——BlockCache。</p><p>客户的读请求会先到memstore中查数据，若查不到就到blockcache中查，再查不到就会从磁盘上读，并把读入的数据同时放入blockcahce。</p><p>HBase的blockcache使用的是LRU（最近最少使用）淘汰策略，当BlockCache的大小达到上限后，会触发缓存淘汰机制，将最老的一批数据淘汰掉。</p><h1 id=reference>Reference</h1><p><a href=https://km.sankuai.com/page/164927192>https://km.sankuai.com/page/164927192</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-f11be7a479d8a489a28499603c436a50>2 - 大数据框架</h1><h1 id=大数据架构概述>大数据架构概述</h1><p><img src=../imgs/bigdata_frame_211212_1.png alt=bigdata_frame_211212_1.png></p><p>实时流计算（Spark/Storm/Flink）</p><p>大数据（Hadoop/HBase/Hive）</p><h1 id=hadoop><strong>Hadoop</strong></h1><h2 id=hadoop生态圈组件><strong>Hadoop生态圈组件</strong></h2><p>1）Zookeeper：是一个开源的分布式应用程序协调服务,基于zookeeper可以实现同步服务，配置维护，命名服务。</p><p>2）Flume：一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。</p><p>3）Hbase：是一个分布式的、面向列的开源数据库, 利用Hadoop HDFS作为其存储系统。</p><p>4）Hive：基于Hadoop的一个数据仓库工具，可以将结构化的数据档映射为一张数据库表，并提供简单的sql 查询功能，可以将sql语句转换为MapReduce任务进行运行。</p><p>5）Sqoop：将一个关系型数据库中的数据导进到Hadoop的 HDFS中，也可以将HDFS的数据导进到关系型数据库中。</p><h2 id=基本概念><strong>基本概念</strong></h2><h3 id=block块-物理划分><strong>block块（物理划分）</strong></h3><p>block是HDFS中的基本存储单位，hadoop1.x默认大小为64M而hadoop2.x默认块大小为128M。</p><h3 id=split分片-逻辑划分><strong>split分片（逻辑划分）</strong></h3><p>Hadoop中split划分属于逻辑上的划分，目的只是为了让map task更好地获取数据。split是通过hadoop中的InputFormat接口中的getSplit()方法得到的。</p><h2 id=mapreduce运行过程概括5步><strong>mapreduce运行过程概括5步</strong></h2><p>1. [input阶段]获取输入数据进行分片作为map的输入</p><p>2. [map阶段]过程对某种输入格式的一条记录解析成一条或多条记录</p><p>3. [shffle阶段]对中间数据的控制，作为reduce的输入</p><p>4. [reduce阶段]对相同key的数据进行合并</p><p>5. [output阶段]按照格式输出到指定目录</p><h2 id=map端shuffle><strong>Map端shuffle</strong></h2><p>①分区partition</p><p>②写入环形内存缓冲区</p><p>③spill：执行溢出写</p><p>        排序sort--->合并combiner--->生成溢出写文件</p><pre><code>如果客户端自定义了Combiner（相当于map阶段的reduce），则会在分区排序后到溢写出前自动调用combiner，将相同的key的value相加，这样的好处就是减少溢写到磁盘的数据量。这个过程叫“合并”
</code></pre><p>④归并merge</p><h2 id=reduce端shuffle><strong>Reduce端shuffle</strong></h2><p>①复制copy</p><p>②归并merge</p><p>③reduce</p><p><strong>MapTask工作机制</strong></p><p>（1）Read阶段：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。</p><p>（2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。</p><p>（3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。</p><p>（4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。</p><p>（5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。</p><h2 id=reducetask工作机制><strong>ReduceTask工作机制</strong></h2><p>（1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。</p><p>（2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。</p><p>（3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。 由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。</p><p>（4）Reduce阶段：reduce()函数将计算结果写到HDFS上。</p><h1 id=hive><strong>Hive</strong></h1><h2 id=hive表关联查询-如何解决数据倾斜的问题><strong>Hive表关联查询，如何解决数据倾斜的问题</strong></h2><p>1）倾斜原因： map输出数据按key Hash的分配到reduce中，由于key分布不均匀、业务数据本身的特、建表时考虑不周、等原因造成的reduce 上的数据量差异过大。</p><p>（1）key分布不均匀;</p><p>（2）业务数据本身的特性;</p><p>（3）建表时考虑不周;</p><p>（4）某些SQL语句本身就有数据倾斜;</p><p>如何避免：对于key为空产生的数据倾斜，可以对其赋予一个随机值。</p><p>2）解决方案</p><p>（1）参数调节：</p><p>hive.map.aggr = true</p><p>hive.groupby.skewindata=true</p><p>生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个Reduce中），最后完成最终的聚合操作。</p><p>（2）SQL 语句调节：</p><p>① 选用join key分布最均匀的表作为驱动表。做好列裁剪和filter操作，以达到两表做join 的时候，数据量相对变小的效果。</p><p>② 大小表Join：</p><p>使用map join让小的维度表（1000 条以下的记录条数）先进内存。在map端完成reduce.</p><p>③ 大表Join大表：</p><p>把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null 值关联不上，处理后并不影响最终结果。</p><p>④ count distinct大量相同特殊值:</p><p>count distinct 时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。</p><h2 id=hive的hsql转换为mapreduce的过程><strong>Hive的HSQL转换为MapReduce的过程</strong></h2><p>HiveSQL ->AST(抽象语法树) -> QB(查询块) ->OperatorTree（操作树）->优化后的操作树->mapreduce任务树->优化后的mapreduce任务树</p><h2 id=hive底层与数据库交互原理><strong>Hive底层与数据库交互原理</strong></h2><p>由于Hive的元数据可能要面临不断地更新、修改和读取操作，所以它显然不适合使用Hadoop文件系统进行存储。目前Hive将元数据存储在RDBMS中，比如存储在MySQL、Derby中。元数据信息包括：存在的表、表的列、权限和更多的其他信息。</p><h2 id=hive的两张表关联-使用mapreduce怎么实现><strong>Hive的两张表关联，使用MapReduce怎么实现</strong></h2><p>1、如果其中有一张表为小表，直接使用map端join的方式（map端加载小表）进行聚合。</p><p>2、如果两张都是大表，那么采用联合key，联合key的第一个组成部分是join on中的公共字段，第二部分是一个flag，0代表表A，1代表表B，由此让Reduce区分客户信息和订单信息；在Mapper中同时处理两张表的信息，将join on公共字段相同的数据划分到同一个分区中，进而传递到一个Reduce中，然后在Reduce中实现聚合。</p><h2 id=hive中sort-by-order-by-cluster-by-distrbute-by><strong>hive中Sort By，Order By，Cluster By，Distrbute By</strong></h2><p>order by：会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）。只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。</p><p>sort by：不是全局排序，其在数据进入reducer前完成排序。</p><p>distribute by：按照指定的字段对数据进行划分输出到不同的reduce中。</p><p>cluster by：除了具有 distribute by 的功能外还兼具 sort by 的功能。</p><h2 id=split-coalesce及collect-list函数><strong>split、coalesce及collect_list函数</strong></h2><p>split将字符串转化为数组，即：split('a,b,c,d' , ',') ==> ["a","b","c","d"]。</p><p>coalesce(T v1, T v2, …) 返回参数中的第一个非空值；如果所有值都为 NULL，那么返回NULL。</p><p>collect_list列出该字段所有的值，不去重 => select collect_list(id) from table</p><h2 id=hive保存元数据方式><strong>Hive保存元数据方式</strong></h2><p>Hive支持三种不同的元存储服务器，分别为：内嵌式元存储服务器、本地元存储服务器、远程元存储服务器，每种存储方式使用不同的配置参数。</p><p>内嵌式元存储主要用于单元测试，在该模式下每次只有一个进程可以连接到元存储，Derby是内嵌式元存储的默认数据库。</p><p>在本地模式下，每个Hive客户端都会打开到数据存储的连接并在该连接上请求SQL查询。</p><p>在远程模式下，所有的Hive客户端都将打开一个到元数据服务器的连接，该服务器依次查询元数据，元数据服务器和客户端之间使用Thrift协议通信。</p><h2 id=hive内部表和外部表区别><strong>Hive内部表和外部表区别</strong></h2><p>创建表时：创建内部表时，会<strong>将数据移动到数据仓库指向的路径</strong>；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。</p><p>删除表时：在删除表的时候，内部表的元数据和数据会被一起删除， 而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。</p><h1 id=hbase><strong>Hbase</strong></h1><h2 id=hbase特点>Hbase特点</h2><ul><li>每个值只出现在一个REGION</li><li>同一时间一个Region只分配给一个Region服务器</li><li>行内的mutation操作都是原子的</li><li>put操作要么成功，要么完全失败</li></ul><p>当某台region server fail的时候，它管理的region failover到其他region server时，需要根据WAL log（Write-Ahead Logging）来redo(redolog，有一种日志文件叫做重做日志文件)，这时候进行redo的region应该是unavailable的，所以hbase降低了可用性，提高了一致性。设想一下，如果redo的region能够响应请求，那么可用性提高了，则必然返回不一致的数据(因为redo可能还没完成)，那么hbase就降低一致性来提高可用性了。</p><h1 id=yarn><strong>Yarn</strong></h1><p>参考：<a href=https://www.jianshu.com/p/3f406cf438be>通俗理解YARN运行原理</a></p><h1 id=流计算对比><strong>流计算对比</strong></h1><h2 id=第一代计算引擎-mapreduce>第一代计算引擎 mapreduce  </h2><p>mapreduce  作为第一个计算引擎，用于批处理，是计算引擎的先驱，内部支持机器学习但是现在机器学习库不在更新，并且mapreduce  编写十分的耗时，开发效率低，开发时间成本太大，所以很少有企业写mapreduce 来跑程序。</p><h2 id=第二代计算引擎-pig-hive>第二代计算引擎 pig/hive</h2><ul><li>作为第二代引擎pig/hive 对hadoop进行了嵌套，其存储基于hdfs，计算基于mr，hive/pig在处理任务时首先会把本身的代码解析为一个个m/r任务，这样就大大的降低了mr的编写编写成本。</li><li>pig 有自己的脚本语言属于，比hive更加的灵活</li><li>hive  属于类sql语法，虽然没有pig灵活，但是对于现在程序员都会sql的世界来说大家更喜欢使用hive</li><li>pig/hive 只支持批处理，且支持机器学习 （hivemall）</li></ul><h2 id=第三代计算引擎-spark-storm>第三代计算引擎 spark/storm</h2><p>随着时代的发展，企业对数据实时处理的需求愈来愈大，所以就出现了storm/spark</p><ul><li>这两者有着自己的计算模式</li><li>storm属于真正的流式处理，低延迟（ms级延迟），高吞吐，且每条数据都会触发计算。</li><li>spark属于批处理转化为流处理即将流式数据根据时间切分成小批次进行计算，对比与storm而言延迟会高于0.5s（s级延迟），但是性能上的消耗低于storm。“流式计算是批次计算的特例（流式计算是拆分计算的结果）”</li></ul><h2 id=第四代计算引擎-flink>第四代计算引擎 flink</h2><ul><li>flink2015年出现在apache，后来又被阿里巴巴技术团队进行优化（这里我身为国人为之自豪）为blink，flink支持流式计算也支持的批次处理。</li><li>flink为流式计算而生属于每一条数据触发计算，在性能的消耗低于storm，吞吐量高于storm，延时低于storm，并且比storm更加易于编写。因为storm如果要实现窗口需要自己编写逻辑，但是flink中有窗口方法。</li><li>flink内部支持多种函数，其中包括窗口函数和各种算子（这一点和spark很像，但是在性能和实时上spark是没有办法比较的）</li><li>flink支持仅一次语义保证数据不丢失</li><li>flink支持通过envent time来控制窗口时间，支持乱序时间和时间处理（这点我觉得很厉害）</li><li>对于批次处理flink的批处理可以理解为 “批次处理是流式处理的特例”（批次计算是流式计算的合并结果）</li></ul><h1 id=spark><strong>Spark</strong></h1><h2 id=rdd><strong>RDD</strong></h2><pre><code>分布式对象集合（有容错机制），本质上是一个只读的分区记录集合。每个 RDD 可以分成多个分区，每个分区就是一个数据集片段。
</code></pre><ul><li>只读：不能修改，只能通过转换操作生成新的 RDD。</li><li>分布式：可以分布在多台机器上进行并行处理。</li><li>弹性：计算过程中内存不够时它会和磁盘进行数据交换。</li><li>基于内存：可以全部或部分缓存在内存中，在多次计算间重用。</li></ul><p><img src=../imgs/bigdata_frame_211212_2.png alt=bigdata_frame_211212_2.png></p><p>Spark Job 默认的调度模式 - FIFO</p><p>RDD 特点 - 可分区/可序列化/可持久化</p><p>Broadcast - 任何函数调用/是只读的/存储在各个节点</p><p>Accumulator - 支持加法/支持数值类型/可并行</p><p>Task 数量由 Partition 决定</p><p>Task 运行在 Workder node 中 Executor 上的工作单元</p><p>master 和 worker 通过 Akka 方式进行通信的</p><p>默认的存储级别 - MEMORY_ONLY</p><p>hive 的元数据存储在 derby 和 MySQL 中有什么区别 - 多会话</p><p>DataFrame 和 RDD 最大的区别 - 多了 schema</p><h2 id=rdd机制><strong>RDD机制</strong></h2><ul><li>分布式弹性数据集，简单的理解成一种数据结构，是spark框架上的通用货币</li><li>所有算子都是基于rdd来执行的</li><li>rdd执行过程中会形成dag图，然后形成lineage保证容错性等</li><li>从物理的角度来看rdd存储的是block和node之间的映射</li></ul><h2 id=shufflemanager-shuffle管理器><strong>ShuffleManager(shuffle管理器)</strong></h2><p>ShuffleManager随着Spark的发展有两种实现的方式，分别为HashShuffleManager和SortShuffleManager，因此spark的Shuffle有Hash Shuffle和Sort Shuffle两种</p><h2 id=spark中的hashshufle的有哪些不足><strong>Spark中的HashShufle的有哪些不足？</strong></h2><ul><li>shuffle产生海量的小文件在磁盘上，此时会产生大量耗时的、低效的IO操作；</li><li>容易导致内存不够用，由于内存需要保存海量的文件操作句柄和临时缓存信息</li><li>容易出现数据倾斜，导致OOM</li></ul><h2 id=spark-hashparitioner的弊端><strong>spark hashParitioner的弊端</strong></h2><ul><li>分区原理：对于给定的key，计算其hashCode</li><li>弊端是数据不均匀，容易导致数据倾斜</li></ul><h2 id=map与flatmap的区别><strong>map与flatMap的区别</strong></h2><ul><li>map：对RDD每个元素转换，文件中的每一行数据返回一个数组对象</li><li>flatMap：对RDD每个元素转换，然后再扁平化，将所有的对象合并为一个对象，会抛弃值为null的值</li></ul><h2 id=union操作是产生宽依赖还是窄依赖><strong>union操作是产生宽依赖还是窄依赖？</strong></h2><ul><li>窄依赖</li></ul><h2 id=常用的action><strong>常用的action</strong></h2><p>collect，reduce,take,count,saveAsTextFile等</p><h2 id=rdd有几种操作类型><strong>rdd有几种操作类型</strong></h2><p>三种：</p><p>1、transformation，rdd由一种转为另一种rdd</p><p>2、action</p><p>3、cronroller，控制算子(cache/persist) 对性能和效率的有很好的支持</p><h2 id=什么场景下要进行persist操作><strong>什么场景下要进行persist操作？</strong></h2><p>以下场景会使用persist</p><ul><li>某个步骤计算非常耗时或计算链条非常长，需要进行persist持久化</li><li>shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大</li><li>shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。</li></ul><h2 id=spark容错机制-血统-lineage-容错><strong>Spark容错机制-血统(Lineage)容错</strong></h2><p>一般来说，分布式数据集的容错性有两种方式：数据检查点和记录数据的更新。</p><pre><code>Lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做进而恢复数据。

相比其他系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据Transformation操作（如filter、map、join等）行为。
</code></pre><h2 id=groupby和groupbykey><strong>groupBy和groupByKey</strong></h2><p>比如（A，1），（A，2）；使用groupBy之后结果是（A，（（A，1），（A，2）））；</p><p>使用groupByKey之后结果是：（A，（1,2））；关键区别就是合并之后是否会自动去掉key信息；</p><h1 id=storm><strong>Storm</strong></h1><h1 id=flink><strong>Flink</strong></h1><h1 id=常见问题><strong>常见问题</strong></h1><h2 id=spark快的原因>spark快的原因</h2><ul><li>Spark基于内存，尽可能的减少了中间结果写入磁盘和不必要的sort、shuffleSpark</li><li>对于反复用到的数据进行了缓存</li><li>Spark对于DAG进行了高度的优化，具体在于Spark划分了不同的stage和使用了延迟计算技术</li></ul><h2 id=spark为什么比mapreduce快><strong>Spark为什么比mapreduce快</strong></h2><p>1、内存（性能高）、磁盘（可靠）</p><p>2、DAG有向无环图在此过程中减少了shuffle以及落地磁盘的次数（一般而言）</p><pre><code>Spark 支持将需要反复用到的数据给 Cache 到内存中，减少数据加载耗时，所以 Spark 跑机器学习算法比较在行（需要对数据进行反复迭代）

Spark的DAG实质上就是把计算和计算之间的编排变得更为细致紧密，使得很多MR任务中需要落盘的非Shuffle操作得以在内存中直接参与后续的运算，并且由于算子粒度和算子之间的逻辑关系使得其易于由框架自动地优化
</code></pre><p>3、Spark是粗粒度资源调度（多线程模型），MapReduce是细粒度资源调度（多进程模型）</p><p>粗粒度资源调度的优点是执行速度快，缺点是不能使集群得到充分的利用；反之亦然。</p><h2 id=mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子><strong>Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子</strong></h2><p>相当于spark中的map算子和reduceByKey算子，区别：MR会自动进行排序的，spark要看具体partitioner</p><h1 id=reference>Reference</h1><p><a href=https://www.cnblogs.com/captainlucky/p/4720986.html>HBase强一致性详解</a></p><p><a href=https://www.jianshu.com/p/7a8fca3838a4>https://www.jianshu.com/p/7a8fca3838a4</a></p><p><a href=https://github.com/Dr11ft/BigDataGuide/blob/master/%E9%9D%A2%E8%AF%95/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89.md>hadoop</a></p><p><a href=https://github.com/Dr11ft/BigDataGuide/blob/master/%E9%9D%A2%E8%AF%95/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94MapReduce.md>hadoop-MapReduce</a></p><p><a href=https://zhuanlan.zhihu.com/p/100258454>https://zhuanlan.zhihu.com/p/100258454</a></p><p><a href=https://blog.csdn.net/JENREY/article/details/84873874>spark为什么比mapreduce快</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-54b122ccb8b94319f23c1def3e2ff60c>3 - HDFS-01基本介绍</h1><h1 id=简介>简介</h1><h2 id=介绍>介绍</h2><p>Hadoop Distributed File System（简称 HDFS）是一个分布式文件系统。HDFS 有着高容错性（fault-tolerent）的特点，并且设计用来部署在低廉的（low-cost）硬件上。而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS 放宽了（relax）POSIX 的要求（requirements）这样可以实现流的形式访问（streaming access）文件系统中的数据。HDFS 开始是为开源的 apache 项目 nutch 的基础结构而创建，HDFS 是 hadoop 项目的一部分，而 hadoop 又是 lucene 的一部分。</p><h2 id=发展历史>发展历史</h2><p>Lucene其实是一个提供全文文本搜索的函数库，它不是一个应用软件。它提供很多API函数，是一个开放源代码的全文检索引擎工具包，让你可以运用到各种实际应用程序中。它提供了完整的查询引擎和索引引擎以及部分的文本分析功能。Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。</p><p>Nutch是一个建立在Lucene核心之上的Web搜索的实现，它是一个真正的应用程序。可以直接下载使用。它在Lucene的基础上加了网络爬虫和一些和Web相关的内容。其目的就是想从一个简单的站内索引和搜索推广到全球网络的搜索上，就像Google和Yahoo一样。Nutch 中还包含了一个分布式文件系统用于存储数据。从 Nutch 0.8.0 版本之后，Doug Cutting 把 Nutch 中的分布式文件系统以及实现 MapReduce 算法的代码独立出来形成了一个新的开源项 Hadoop。Nutch 也演化为基于 Lucene 全文检索以及 Hadoop 分布式计算平台的一个开源搜索引擎。</p><h2 id=应用场景>应用场景</h2><h1 id=名词解释>名词解释</h1><p><strong>Hadoop</strong>：一个分布式系统基础架构，由Apache基金会开发。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力高速运算和存储。</p><p><strong>Distributed</strong>：分布式计算是利用互联网上的计算机的 CPU 的共同处理能力来解决大型计算问题的一种计算科学。</p><p><strong>File system</strong>：文件系统是操作系统用于明确磁盘或分区上的文件的方法和数据结构；即在磁盘上组织文件的方法。也指用于存储文件的磁盘或分区，或文件系统种类。</p><h1 id=架构>架构</h1><p><img src=../imgs/20230104_hdfs01_1.png alt=20230104_hdfs01_1.png></p><p>架构主要由四个部分组成，分别为HDFS Client、NameNode、DataNode和Secondary NameNode。下面分别介绍这四个组成部分。</p><h2 id=hdfs-client>HDFS Client</h2><p>和 HDFS 打交道是通过客户端，无论读取一个文件或者写一个文件，都是把数据交给 HDFS client，它负责和 Name nodes 以及 Data nodes 联系并传输数据。</p><p>主要功能如下：</p><ul><li>文件切分。文件上传 HDFS 时，将文件切分成一个一个的 Block，然后进行存储；</li><li>与 NameNode 交互，获取文件的位置信息；</li><li>与 DataNode 交互，读取或者写入数据；</li><li>Client 提供一些命令来管理 HDFS，比如启动或者关闭HDFS；</li><li>Client 可以通过一些命令来访问 HDFS；</li></ul><h2 id=namenode>NameNode</h2><p>也就是Master，它是一个管理者。</p><p>主要功能如下：</p><ul><li>管理 HDFS 的名称空间；</li><li>处理客户端读写请求</li><li>管理 DataNode 回报的数据块（Block）映射信息；</li><li>配置副本策略。</li></ul><h2 id=datanode>DataNode</h2><p>可以理解为 Slave。DataNode 是 Block 真正存储的地方。DataNode的本地磁盘以文件形式存储着Block信息。同时还存储着Block的元数据信息文件。元数据主要存储MD5值用来进行验证</p><p>HDFS在启动时，DataNode 会向 NameNode汇报 block的信息。</p><p>DataNode通过向NameNode发送心跳保持与其联系（3秒一次），如果 NameNode 10 分钟没有收到 DataNode 的心跳，则认为其已经 lost，并复制其上的 block 到其它 DataNode。</p><p>主要功能如下：</p><ul><li>存储实际的数据块；</li><li>执行数据块的读/写操作。</li></ul><h2 id=secondarynamenode>SecondaryNameNode</h2><p>并非NameNode的热备。当 NameNode 挂掉时，并不能马上替换NameNode并提供服务。备用NameNode 通常在与 主NameNode 不同的计算机上运行，它的内存要求与 主NameNode 的相同。</p><p>主要功能如下：</p><ul><li>辅助NameNode，分担其工作量；</li></ul><blockquote><p>启动备用 NameNode 时，会从映像文件 fsimage 中读取 HDFS 状态，然后启用“编辑日志文件”对它进行编辑。然后将新的HDFS状态写入fsimage，并使用“空编辑文件”启动正常操作。</p></blockquote><ul><li>定期合并 Fsimage 和 Edits，并推送给 NameNode；</li></ul><blockquote><p>由于 NameNode 只在启动时合并 fsimage 和编辑文件，随着时间推移，“编辑日志文件”会变得非常大。导致在下次重新启动 NameNode 时需要花费更长的时间。备用NameNode 定期合并 fsimage 和“编辑日志文件”，并将“编辑日志文件”的大小保持在限定范围内。减少 NameNode 启动时间</p></blockquote><ul><li>紧急情况下，可辅助恢复NameNode。</li></ul><h2 id=物理拓扑>物理拓扑</h2><p>至少分为三层：</p><ul><li>顶层交换机</li><li>机架</li><li>服务器
hdfs具备机架感知，可以感知集群的物理拓扑，因此在数据副本放置的时候可以根据物理拓扑进行分配，同时能够为用户就近选择读写的datanode节点。</li></ul><h1 id=数据分布>数据分布</h1><h2 id=副本分布>副本分布</h2><p>hdfs拓扑模块具备机架感知功能，这个功能好处在于能够让client从最近的存储节点读数据，也能够在数据副本复制的时候按照从近到远的方式复制，提升总体带宽，同时也能够使副本放置尽可能合理，从而提高数据可靠性。 数据副本放置策略对于数据的可靠性、读写性能、可用性影响较大。</p><p>hdfs数据副本在client请求新的Block时由NameNode确定其存放在哪些DataNode节点，hdfs默认的副本分配方式是将第一个副本放置在离client最近的DataNode节点，其他两个副本放在不同的机架上。在充分保证读写性能的同时尽可能的保证最大的可靠性和可用性。 hdfs的副本放置可以总结为两点:</p><ol><li>一个DataNode上不会出现一个Block的两个副本；</li><li>一个机架上不会存储一个Block的三个及以上的副本（前提：机架数量充足）。</li></ol><h2 id=副本管理>副本管理</h2><p>hdfs的副本管理粒度是以Block为单位的，Block大小为128MB（hadoop 1.x都是64MB，hadoop 2.x都是128MB），如果副本数量为3，那么一个Block就至少需要BlockID + 3*DataNodeID这样大小的元数据，这与当前很多流行的分布式系统设计是不一样的，比如tikv，其副本管理单位是一个raft group，这个raft group管理多个block，通常一个raft group管理的数据量大小在数十G规模。这也是目前很多分布式系统常用的副本管理方式。 hdfs的Block是动态创建的，client向NameNode申请新的block，NameNode会分配一个新的BlockID并为这个Block分配三个DataNode节点，用作副本存放。</p><h1 id=数据一致性>数据一致性</h1><p>hdfs只支持一写多读模式，这种模式简化了数据一致性的设计，因为不需要在client之间同步写入状态了，cephfs支持多写多读，其多个client之间的状态同步比较复杂。 另外hdfs的文件只支持追加写入，这同样有利于数据一致性的设计实现，当然这种只支持追加写的模式也是与其应用场景相结合的。同时仅支持追加写对于带宽也是友好的。</p><h2 id=数据复制>数据复制</h2><p>hdfs的数据复制以pipeline方式进行，数据从client发到与其最近的DataNode节点，然后由第一个DataNode节点复制给第二个DataNode节点，这样以此类推，每个package的ack按照复制方向的反方向流动，最终返回给client。</p><h2 id=写数据一致性>写数据一致性</h2><p>hdfs保证同一时间只有一个client可以写文件，同时可见性只是在文件close和用户显示调用flush的时候。如果只是正常的写入返回并不保证写入的数据对用户可见，这个与文件创建时其配置有一定关系，具体可参考<a href=https://zhuanlan.zhihu.com/p/102474646/edit#https://stackoverflow.com/questions/37533600/whats-the-hdfs-writing-consistency>what's the HDFS writing consistency</a>。</p><h2 id=读数据一致性>读数据一致性</h2><p>对于同一个client，hdfs保证“read your write”一致性语义，实现方式主要是通过记录client的写状态ID，在执行读请求时会携带这个ID，这个ID会发给NamaNode，由NameNode保证在允许其读请求执行之前其写请求已经被执行。 对于多个client，hdfs提供msync调用，在读取非自身写的时候，先执行msync，msync会刷新NameNode上其自身的状态ID，使其ID保持最新状态，能够读到其他client写入的最新数据。</p><h1 id=工作流程>工作流程</h1><h2 id=写数据过程>写数据过程</h2><p><img src=../imgs/20230104_hdfs01_2.png alt=20230104_hdfs01_2.png></p><ol><li>客户端通过 Distributed FileSystem 模块向 NameNode 请求上传文件，NameNode 检查目标文件是否已存在，父目录是否存在。</li><li>NameNode 返回是否可以上传。</li><li>客户端请求第一个 block 上传到哪几个 datanode 服务器上。</li><li>NameNode 返回 3 个 datanode 节点，分别为 dn1、dn2、dn3。</li><li>客户端通过 FSDataOutputStream 模块请求 dn1 上传数据，dn1 收到请求会继续调用 dn2，然后 dn2 调用 dn3，通信管道建立完成。</li><li>dn1、dn2、dn3 逐级应答客户端。</li><li>客户端开始往 dn1 上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以 packet 为单位，dn1 收到一个 packet 就会传给 dn2，dn2 传给 dn3；dn1 每传一个packet 会放入一个应答队列等待应答。</li><li>当一个block传输完成之后，客户端再次请求NameNode上传第二个block的服务器。（重复执行3-7步）。</li></ol><h2 id=读数据过程>读数据过程</h2><p><img src=../imgs/20230104_hdfs01_3.png alt=20230104_hdfs01_3.png></p><ol><li>客户端通过 Distributed FileSystem 向 NameNode 请求下载文件，NameNode 通过查询元数据，找到文件块所在的DataNode地址。</li><li>挑选一台 DataNode（就近原则，然后随机）服务器，请求读取数据。</li><li>DataNode 开始传输数据给客户端（从磁盘里面读取数据输入流，以 packet 为单位来做校验）。</li><li>客户端以 packet 为单位接收，先在本地缓存，然后写入目标文件。</li></ol><h2 id=second-namenode工作机制>Second NameNode工作机制</h2><p><img src=../imgs/20230104_hdfs01_4.png alt=20230104_hdfs01_4.png></p><p>第一阶段：NameNode启动</p><ol><li><p>第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。</p></li><li><p>客户端对元数据进行增删改的请求。</p></li><li><p>NameNode记录操作日志，更新滚动日志。</p></li><li><p>NameNode在内存中对数据进行增删改查。
第二阶段：Secondary NameNode工作</p></li><li><p>Secondary NameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。</p></li><li><p>Secondary NameNode请求执行checkpoint。</p></li><li><p>NameNode滚动正在写的edits日志。</p></li><li><p>将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。</p></li><li><p>Secondary NameNode加载编辑日志和镜像文件到内存，并合并。</p></li><li><p>生成新的镜像文件fsimage.chkpoint。</p></li><li><p>拷贝fsimage.chkpoint到NameNode。</p></li><li><p>NameNode将fsimage.chkpoint重新命名成fsimage。</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-13f4b5c2b3adc66b901f3693d67aab0f>4 - HDFS-02基本命令</h1><h1 id=hdfs-dfs与hadoop-fs>hdfs dfs与hadoop fs</h1><p>HDFS的命令实际上和Linux命令相似，基本就是在 <code>hdfs dfs -</code> 加上Linux命令即可（ <code>hdfs dfs -ls /</code> 和 <code>hadoop fs -ls /</code> 效果一样）</p><h1 id=hadoop命令>hadoop命令</h1><h2 id=打印配置路径>打印配置路径</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hadoop classpath <span style=color:#177500># 打印当前环境的配置路径</span>
</span></span></code></pre></td></tr></table></div></div></div><h1 id=hdfs命令>HDFS命令</h1><h2 id=查看帮助>查看帮助</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hdfs dfs -help
</span></span></code></pre></td></tr></table></div></div></div><h2 id=查看当前目录信息>查看当前目录信息</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hdfs dfs -ls /
</span></span></code></pre></td></tr></table></div></div></div><h2 id=上传文件>上传文件</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hdfs dfs -put /local_path /hdfs_path
</span></span></code></pre></td></tr></table></div></div></div><h2 id=剪切文件>剪切文件</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hdfs dfs -moveFromLocal test.txt /test1.txt
</span></span></code></pre></td></tr></table></div></div></div><h2 id=合并下载>合并下载</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hdfs dfs -getmerge /hdfs_dir /merged_file
</span></span></code></pre></td></tr></table></div></div></div><h2 id=创建文件夹>创建文件夹</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hdfs dfs -mkdir /test
</span></span></code></pre></td></tr></table></div></div></div><h2 id=移动文件>移动文件</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hdfs dfs -mv /hdfs_dir1 /hdfs_dir2
</span></span></code></pre></td></tr></table></div></div></div><h2 id=复制文件>复制文件</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hdfs dfs -cp /hdfs_dir1 /hdfs_dir2
</span></span></code></pre></td></tr></table></div></div></div><h2 id=删除文件>删除文件</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hdfs dfs -rm /test.txt
</span></span></code></pre></td></tr></table></div></div></div><h2 id=查看文件>查看文件</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hdfs dfs -cat /test.txt
</span></span><span style=display:flex><span>hdfs dfs -tail -f /test.txt
</span></span></code></pre></td></tr></table></div></div></div><h2 id=查看文件数量>查看文件数量</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hdfs dfs -count /test
</span></span></code></pre></td></tr></table></div></div></div><h2 id=查看空间>查看空间</h2><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>hdfs dfs -df /
</span></span><span style=display:flex><span>hdfs dfs -df -h /
</span></span></code></pre></td></tr></table></div></div></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c937e020882673d7b1c7e8747009259e>5 - Spark</h1><h1 id=introduction>Introduction</h1><p>Spark intro</p></div><div class=td-content><h1 id=pg-003faf893f62ece2e3e51f9693a847bd>5.1 - 01.Spark基本介绍</h1><h1 id=概述>概述</h1><p>Spark 是 UC Berkeley AMP Lab 开源的通用分布式并行计算框架，目前已成为 Apache 软件基金会的顶级开源项目。Spark 支持多种编程语言，包括 Java、Python、R 和 Scala，同时 Spark 也支持 Hadoop 的底层存储系统 HDFS，但 Spark 不依赖 Hadoop。</p><h1 id=spark优势>Spark优势</h1><ul><li><p>高效性
Spark 比 MapReduce 快100倍。</p><ul><li>Spark基于内存存储中间计算结果，减少了中间结果写入磁盘的IO和不必要的sort、shuffle</li><li>Spark对于反复用到的数据进行了缓存</li><li>Spark通过并行计算DAG图的优化，具体在于Spark划分了不同的stage，减少了不同任务之间的依赖，使用延迟计算技术降低了延迟等待时间</li></ul></li><li><p>易用性
不同于MapReduce仅支持Map和Reduce两种编程算子，Spark提供了超过80种不同的Transformation和Action算子，如map、reduce、filter、groupByKey、sortByKey、foreach等，并且采用函数式编程风格，实现相同的功能需要的代码量极大缩小。</p></li><li><p>通用性
Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。</p></li><li><p>兼容性
Spark能够跟很多开源工程兼容使用。如Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且Spark可以读取多种数据源，如HDFS、HBase、MySQL等。</p></li></ul><h1 id=spark基本概念>Spark基本概念</h1><p>Spark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎</p><p><strong>RDD</strong>：是弹性分布式数据集（Resilient Distributed Dataset）的简称，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型。</p><p><strong>DAG</strong>：是Directed Acyclic Graph（有向无环图）的简称，反映RDD之间的依赖关系。</p><p><strong>Driver Program</strong>：控制程序，负责为Application构建DAG图。</p><p><strong>Cluster Manager</strong>：集群资源管理中心，负责分配计算资源。</p><p><strong>Worker Node</strong>：工作节点，负责完成具体计算。</p><p><strong>Executor</strong>：是运行在工作节点（Worker Node）上的一个进程，负责运行Task，并为应用程序存储数据。</p><p><strong>Application</strong>：用户编写的Spark应用程序，一个Application包含多个Job。</p><p><strong>Job</strong>：作业，一个Job包含多个RDD及作用于相应RDD上的各种操作。</p><p><strong>Stage</strong>：阶段，是作业的基本调度单位，一个作业会分为多组任务，每组任务被称为“阶段”。</p><p><strong>Task</strong>：任务，运行在Executor上的工作单元，是Executor中的一个线程。</p><p>Application由多个Job组成，Job由多个Stage组成，Stage由多个Task组成。Stage是作业调度的基本单位。</p><h2 id=rdd>RDD</h2><p>RDD（Resilient Distributed Dataset）是Spark中最基础的数据结构之一，它是一个可分区、可并行操作、容错的数据集合，可以跨集群进行分布式计算。RDD的每个分区存储在不同的节点上，且每个分区都可以被处理器并行计算，以实现分布式计算的目的。</p><h2 id=dataframe>DataFrame</h2><p>DataFrame是Spark SQL中的一种数据结构，是一个带有命名列的分布式数据集合。与RDD不同，DataFrame具有模式（Schema）信息，可以用于结构化数据的处理和分析。DataFrame也支持类似SQL的查询操作，可以通过Spark SQL或DataFrame API进行操作。</p><h2 id=dataset>DataSet</h2><p>Dataset是Spark 1.6版本中引入的一种数据结构，是DataFrame的强类型版本。与DataFrame不同，Dataset具有编译时类型检查和类型安全性，并且可以通过Scala和Java中的Lambda表达式进行操作。Dataset既可以像DataFrame一样进行结构化数据处理，也可以像RDD一样进行函数式编程。</p><h1 id=spark核心模块>Spark核心模块</h1><p><img src=../imgs/spark_intro_20230314_1.png alt=spark_intro_20230314_1.png></p><h2 id=sparkcore>SparkCore</h2><p>SparkCore中提供了Spark最基础与最核心的功能, Spark其他的功能如: SparkSQL,  SparkStreaming, GraphX, MLlib# 8276SparkCore的基础上进行扩展的</p><h2 id=sparksql>SparkSQL</h2><p>SparkSQLJSparkFICHE(EzeFa(CSRAMAAiitSparkSQL, 用户可以使用SQL或者ApacheHive版本的SQL方言(HQL) 来查询数据。</p><h2 id=sparkstreaming>SparkStreaming</h2><p>SparkStreaming是Spark平台上针对实时数据进行流式计算的组件, 提供了丰富的处理</p><p>数据流的API。</p><h2 id=sparkmllib>SparkMLlib</h2><p>MLlib是Spark中用于机器学习的组件，它包括了许多常见的机器学习算法和工具，如分类、回归、聚类、降维等。MLlib的算法可以处理大规模的数据集合，并且可以通过RDD、DataFrame和Dataset进行操作。</p><h1 id=spark架构设计>Spark架构设计</h1><h2 id=整体架构>整体架构</h2><p>Spark集群由以下部分组成：</p><ul><li>Driver</li><li>Cluster Manager（Standalone,Yarn 或 Mesos）</li><li>Worker Node
对于每个Spark应用程序，Worker Node上存在一个Executor进程，Executor进程中包括多个Task线程。</li></ul><p><img src=../imgs/spark_intro_20230314_2.png alt=spark_intro_20230314_2.png></p><h1 id=spark核心流程>Spark核心流程</h1><h2 id=启动流程>启动流程</h2><p>启动Spark时，Spark将经历以下步骤：</p><ol><li>Spark应用程序的入口点是Spark的驱动程序。驱动程序是一个包含main函数的JVM进程。当驱动程序运行时，它创建一个SparkContext对象。这个对象是与Spark集群通信的主要途径，它包含了所有集群的配置信息。</li><li>SparkContext对象与集群管理器通信，并请求资源来执行Spark应用程序。集群管理器可以是Standalone、YARN或Mesos。当SparkContext向集群管理器发出请求时，它将分配给应用程序一个或多个执行器进程。</li><li>执行器进程启动后，它们将向SparkContext注册，并等待接收任务。当Spark应用程序运行时，SparkContext将根据需要将任务发送给执行器进程。</li><li>在运行期间，Spark应用程序将创建RDD（Resilient Distributed Datasets）并对它们执行操作。RDD是Spark的核心抽象，它是一个分布式的、可缓存的、不可变的数据集合。应用程序可以从文件、Hive表、数据库或内存数据结构等数据源创建RDD，然后对它们执行各种转换操作（如map、filter、reduceByKey等）以生成新的RDD。</li><li>应用程序也可以将RDD存储到磁盘上，以便在以后的运行中重复使用。Spark支持不同类型的存储系统，包括HDFS、S3、Cassandra、HBase等。</li><li>当应用程序完成时，SparkContext将释放资源并关闭执行器进程。应用程序也可以手动停止SparkContext来终止应用程序。</li></ol><h2 id=执行流程>执行流程</h2><p>Apache Spark是一个用于分布式数据处理的计算引擎，其执行流程主要包括以下步骤：</p><ol><li>应用程序创建SparkContext对象：应用程序通过创建SparkContext对象来连接到Spark集群。SparkContext对象充当与集群交互的主要接口。</li><li>读取数据并创建RDD：Spark将数据读入内存中，并将其表示为弹性分布式数据集(RDD)。RDD是Spark中的基本数据结构，它允许数据在集群中的多个节点之间进行分布式处理。</li><li>转换RDD：应用程序可以对RDD进行转换，以执行各种操作。转换操作是惰性的，这意味着它们只有在执行操作时才会被计算。</li><li>缓存RDD：Spark支持将RDD缓存在内存中，以便反复使用。缓存可以提高性能，因为它可以减少I/O和计算开销。</li><li>执行操作：当应用程序调用操作时，Spark将转换操作应用于RDD，并在集群中的多个节点上执行计算。Spark使用任务来执行计算，并将任务分发给集群中的多个节点。</li><li>保存结果：一旦计算完成，应用程序可以将结果保存到磁盘或将其返回给驱动程序。
总之，Spark的执行流程涉及将数据加载到内存中，并使用RDD进行转换和操作，最终将结果保存到磁盘或返回给驱动程序。Spark执行计算时利用集群中多个节点的并行计算能力，从而实现高性能和高可伸缩性。</li></ol><h2 id=shuffle>Shuffle</h2><p>在Spark中，Shuffle是指将RDD（Resilient Distributed Dataset）中的数据重新分区，以便在不同节点上进行数据聚合或计算的过程。Shuffle操作是一种非常耗时和开销大的操作，因为它需要将数据从不同节点的Task中读取、序列化、写入磁盘、再从磁盘读取、反序列化等过程。</p><p>Spark中的Shuffle可以分为两种类型：</p><ol><li><p>基于Hash的Shuffle：使用Hash函数将Key相同的记录映射到同一个分区中。由于Hash函数的随机性，Hash Shuffle通常可以比较好地实现数据的均衡分布。</p></li><li><p>基于Sort的Shuffle：使用Key值进行排序，将Key相同的记录分配到同一个分区中。由于Sort Shuffle需要先进行排序，因此其开销一般会比Hash Shuffle更高。
Spark Shuffle操作可以发生在多个阶段，包括Map端Shuffle和Reduce端Shuffle：</p></li><li><p>Map端Shuffle：在Map阶段，Shuffle操作是通过将Mapper任务的输出数据分区、排序、写入磁盘，再由Reduce任务读取磁盘上的分区文件进行Reduce操作。</p></li><li><p>Reduce端Shuffle：在Reduce阶段，Shuffle操作是通过将Mapper任务的输出数据写入内存中的缓冲区，当缓冲区达到一定的大小时，将缓冲区中的数据分区、排序、写入磁盘，最后由Reduce任务读取磁盘上的分区文件进行Reduce操作。
为了减少Shuffle操作的开销，Spark提供了一些优化方法，包括：</p></li><li><p>使用CombineByKey或reduceByKey等算子，在Map端进行局部聚合，减少Reduce端Shuffle的数据量。</p></li><li><p>使用Spark SQL的DataFrame或Dataset API，尽量使用Spark的内置优化，减少Shuffle的开销。</p></li><li><p>调整Spark SQL的shuffle分区数，适当增大shuffle分区数可以减少数据倾斜的问题。</p></li><li><p>将需要缓存的RDD或DataFrame先进行缓存，避免多次计算时导致Shuffle的开销。
综上所述，Shuffle是Spark中一个重要的操作，其开销很大，需要谨慎使用。针对具体的应用场景和需求，可以采用不同的优化策略来减少Shuffle的开销。</p></li></ol><h2 id=数据倾斜>数据倾斜</h2><p>Spark数据倾斜是指在数据分布不平衡的情况下，某些节点的数据处理时间远远长于其他节点，导致整个Spark应用程序的执行时间变慢。解决Spark数据倾斜的常见方法包括以下几个方面：</p><ol><li>了解数据分布情况：首先需要了解数据分布的情况，通过查看数据的Key分布情况、数据大小、数据来源等信息，以及对任务进行Profile和监控，可以快速定位数据倾斜的问题。</li><li>重新分区：对于数据倾斜的情况，可以考虑重新分区，使得数据均匀分布在多个分区中，可以使用repartition或coalesce函数重新分区。</li><li>增加shuffle操作的并行度：对于包含shuffle操作的任务，可以增加其并行度，使得数据可以更均匀地分布到不同的节点上。可以通过调整spark.sql.shuffle.partitions参数来控制shuffle操作的并行度。</li><li>使用随机前缀：对于Key值分布不均的情况，可以使用随机前缀的方式来将数据打散到不同的分区中。通过增加随机前缀可以使得原本的Key值发生变化，从而均匀分布在不同的分区中。</li><li>增加缓存：对于一些常用的RDD或DataFrame，可以将其缓存起来，避免多次计算导致数据倾斜。缓存操作可以使用persist或cache函数。</li><li>使用广播变量：对于一些较小的数据集，可以使用广播变量将其广播到所有节点上，避免在每个节点上都重新计算一遍。广播变量可以使用broadcast函数。</li><li>使用动态调整算子：对于部分RDD数据倾斜的情况，可以使用动态调整算子，将数据分散到多个节点上进行处理。可以使用zipWithIndex函数实现动态调整算子。
综上所述，Spark数据倾斜的解决方法主要包括重新分区、增加shuffle操作的并行度、使用随机前缀、增加缓存、使用广播变量、使用动态调整算子等。针对具体问题，需要根据具体情况采用不同的解决方法。</li></ol><h1 id=rdd-1>RDD</h1><p>Spark将数据保存分布式内存中，对分布式内存的抽象理解，提供了一个高度受限的内存模型，RDD在逻辑上集中，物理上存储在集群的多台机器上。</p><h2 id=特点>特点</h2><p>RDD的特点包括：</p><ol><li>弹性：RDD具有容错性，如果某个节点宕机，Spark可以根据RDD的依赖关系重新计算该节点上的数据，从而保证了程序的健壮性。</li><li>分区：RDD将数据集合按照一定规则进行分区，以便并行处理，不同的分区可以被不同的计算节点处理。</li><li>延迟计算：RDD采用延迟计算的方式，只有在需要计算结果时才会进行真正的计算，这可以有效地减少不必要的计算开销。</li><li>缓存：Spark可以将RDD缓存在内存中，避免重复计算，提高计算效率。</li><li>支持多种数据源：RDD支持多种数据源，包括本地文件系统、Hadoop文件系统、HBase、Cassandra等。
在Spark中，RDD是一种不可变的数据结构，意味着一旦创建就不能被修改，只能通过转换操作生成新的RDD。RDD的操作可以分为两类：转换操作和行动操作。转换操作是指对RDD进行某些计算，生成新的RDD，但并不会立即执行，只有在行动操作被调用时才会执行计算。行动操作是指触发RDD计算并返回结果的操作，如count、collect、reduce等。</li></ol><p>总之，RDD是Spark中非常重要的一种数据结构，它可以实现分布式的数据处理，并具有容错性、弹性、延迟计算等特点。熟练掌握RDD的使用可以帮助我们更好地进行分布式计算。</p><h2 id=创建方式>创建方式</h2><p>RDD可以由两种方式创建：</p><ol><li>从已有数据集合（如Hadoop文件、本地文件、数据库等）创建，可以通过SparkContext的textFile等函数创建。如：使用程序中的集合创建、使用本地文件系统创建、使用hdfs创建、基于数据库db创建、基于Nosql创建（如hbase）、基于s3创建、基于数据流（如socket创建）</li><li>通过转换已有的RDD，生成新的RDD，可以通过Map、Filter、GroupBy等函数进行操作。</li></ol><h2 id=特征>特征</h2><ul><li><p>由一系列parition组成
RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p></li><li><p>算子（函数）是作用在partition上
Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p></li><li><p>RDD之间有依赖关系
RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。当部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p></li><li><p>分区器作用在KV格式的RDD上
Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p></li><li><p>Partition提供数据最佳的计算位置，有利于数据处理的本地化。
对于一个HDFS文件，列表保存的是每个Partition所在块的位置。按照“移动数据不如移动计算”的理念，Spark在任务调度时，会尽可能将计算任务分配到其所要处理数据块的存储位置。</p></li></ul><h2 id=窄依赖和宽依赖>窄依赖和宽依赖</h2><p>窄依赖（一对一）：没有数据的shuffling，所有的父RDD的partition会一一映射到子RDD的partition中</p><p>宽依赖（一对多）：发生数据的shuffling，父RDD中的partition会根据key的不同进行切分，划分到子RDD中对应的partition中</p><h1 id=spark部署模式>Spark部署模式</h1><p>Spark作为一个数据处理框架和计算引擎，被设计在所有常见的集群环境中运行，在国内工作中主流的环境为Yarn，不过逐渐容器式环境也慢慢流行起来。</p><h2 id=local模式>Local模式</h2><p>想啥呢, 你之前一直在使用的模式可不是Local模式哟。所谓的Local模式, 就是不需要其他任何节点资源就可以在本地执行麓F代码的坏境，一般用于教学, 调试, 演示等, 之前在IDEA中运行代码的环境我们称之为开发环境, 不太一样。</p><h3 id=解压缩文件>解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到Linux并解压缩, 放置在指定位置。</p><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
</span></span><span style=display:flex><span><span style=color:#a90d91>cd</span> /opt/module
</span></span><span style=display:flex><span>mv spark-3.0.0-bin-hadoop3.2 spark-local
</span></span></code></pre></td></tr></table></div></div></div><h3 id=启动local环境>启动Local环境</h3><p>进入解压缩后的路径，执行如下命令</p><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>bin/spark-shell
</span></span></code></pre></td></tr></table></div></div></div><h3 id=提交应用>提交应用</h3><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>bin/spark-submit --class org.apache.spark.examples.SparkPi --master local<span style=color:#000>[</span>2<span style=color:#000>]</span> ./examples/jars/spark-examples_2.12-3.0.0.jar <span style=color:#1c01ce>10</span>
</span></span></code></pre></td></tr></table></div></div></div><h2 id=standalone模式>Standalone模式</h2><p>local本地模式毕竟只是用来进行练习演示的, 真实工作中还是要将应用提交到对应的集群中去执行, 这里我们来看看只使用Spark自身节点运行的集群模式, 也就是我们所谓的独立部署(Standalone) 模式。Spark的Standalone模式体现了经典的mastel-slave模式。</p><p>集群规划: </p><table><thead><tr><th style=text-align:left></th><th style=text-align:left>Linux1</th><th style=text-align:left>Linux2</th><th style=text-align:left>Linux3</th></tr></thead><tbody><tr><td style=text-align:left>Spark</td><td style=text-align:left>Worker Master</td><td style=text-align:left>Worker</td><td style=text-align:left>Worker</td></tr></tbody></table><h3 id=解压缩文件-1>解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到Linux并解压缩, 放置在指定位置。</p><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
</span></span><span style=display:flex><span><span style=color:#a90d91>cd</span> /opt/module
</span></span><span style=display:flex><span>mv spark-3.0.0-bin-hadoop3.2 spark-standalone
</span></span></code></pre></td></tr></table></div></div></div><h3 id=修改配置文件>修改配置文件</h3><p>1）进入解压缩路径的conf目录，修改salves.tempalte文件名为slaves</p><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>mv slaves.template slaves
</span></span></code></pre></td></tr></table></div></div></div><p>2）修改slaves文件，添加work节点<style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-xml data-lang=xml><span style=display:flex><span>linux1
</span></span><span style=display:flex><span>linux2
</span></span><span style=display:flex><span>linux3
</span></span></code></pre></td></tr></table></div></div></div></p><p>3）修改spark-env.sh.template文件名为spark-env.sh<style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>mv spark-env.sh.template spark-env.sh
</span></span></code></pre></td></tr></table></div></div></div></p><p>4）修改spark-env.sh文件，添加JAVA_HOME环境变量和集群对应的master节点<style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-xml data-lang=xml><span style=display:flex><span>export JAVA_HOME=/opt/mudule/jdk1.8.0_144
</span></span><span style=display:flex><span>SPARK_MASTER_HOST=linux1
</span></span><span style=display:flex><span>SPARK_MASTER_PORT=7077
</span></span></code></pre></td></tr></table></div></div></div></p><p>注意：7077端口，相当于hadoop3内部通信的8020端口，此处的端口需要确认自己的Hadoop配置
5）分发spark-standalone目录</p><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>xsync spark-standalone
</span></span></code></pre></td></tr></table></div></div></div><h3 id=启动集群>启动集群</h3><p>1）执行命令</p><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>sbin/start-all.sh
</span></span></code></pre></td></tr></table></div></div></div><p>2）查看三台服务器运行进行<style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>xcall jps
</span></span></code></pre></td></tr></table></div></div></div></p><p>3）查看Master资源监控Web UI界面：<a href=http://linux1%EF%BC%9A8080>http://linux1</a>:8080</p><h3 id=提交应用-1>提交应用</h3><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://linux1:7077 ./examples/jars/spark-examples_2.12-3.0.0.jar <span style=color:#1c01ce>10</span>
</span></span></code></pre></td></tr></table></div></div></div><h2 id=yarn模式>Yarn模式</h2><p>独立部署(Standalone) 模式由Spark自身提供计算资源, 无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性, 独立性非常强。但是你也要记住, Spark主要是计算框架, 而不是资源调度框架. 以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的Yarn坏境下Spark是如何工作的(其实是因为在国内工作中, Yarn使用的非常多) 。</p><h3 id=解压缩文件-2>解压缩文件</h3><p>将spark-3.0.0-bin-hadoop3.2.tgz文件上传到Linux并解压缩, 放置在指定位置。</p><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>tar -zxvf spark-3.0.0-bin-hadoop3.2.tgz -C /opt/module
</span></span><span style=display:flex><span><span style=color:#a90d91>cd</span> /opt/module
</span></span><span style=display:flex><span>mv spark-3.0.0-bin-hadoop3.2 spark-yarn
</span></span></code></pre></td></tr></table></div></div></div><h3 id=修改配置文件-1>修改配置文件</h3><p>1）修改hadoop配置文件/opt/module/hadoop/etc/hadoop/yarn-site.xml，并分发</p><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">11
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-xml data-lang=xml><span style=display:flex><span><span style=color:#177500>&lt;!-- 是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span>
</span></span><span style=display:flex><span><span style=color:#000>&lt;property&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#000>&lt;name&gt;</span>yarn.nodemanager.pmem-check-enabled<span style=color:#000>&lt;/name&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#000>&lt;value&gt;</span>false<span style=color:#000>&lt;/value&gt;</span>
</span></span><span style=display:flex><span><span style=color:#000>&lt;/property&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#177500>&lt;!-- 是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span>
</span></span><span style=display:flex><span><span style=color:#000>&lt;property&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#000>&lt;name&gt;</span>yarn.nodemanager.vmem-check-enabled<span style=color:#000>&lt;/name&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#000>&lt;value&gt;</span>false<span style=color:#000>&lt;/value&gt;</span>
</span></span><span style=display:flex><span><span style=color:#000>&lt;/property&gt;</span>
</span></span></code></pre></td></tr></table></div></div></div><p>2）修改conf/spark-env.sh，添加JAVA_HOME和YARN_CONF_DIR配置<style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">3
</span><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">4
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>mv spark-env.sh.template spark-env.sh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a90d91>export</span> <span style=color:#000>JAVA_HOME</span><span style=color:#000>=</span>/opt/module/jdk1.8.0_144
</span></span><span style=display:flex><span><span style=color:#000>YARN_CONF_DIR</span><span style=color:#000>=</span>/opt/module/hadoop/etc/hadoop
</span></span></code></pre></td></tr></table></div></div></div></p><h3 id=启动hdfs和yarn集群>启动HDFS和YARN集群</h3><p>略</p><h3 id=提交应用-2>提交应用</h3><style>.td-content .highlight{margin-top:.5rem;margin-bottom:.5rem}.code-collapse1{overflow-y:auto;max-height:500px;overflow-x:auto;max-width:100%}</style><div class=code-collapse1><div class=highlight><div style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><table style=border-spacing:0;padding:0;margin:0;border:0><tr><td style=vertical-align:top;padding:0;margin:0;border:0><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code><span style="white-space:pre;user-select:none;margin-right:.4em;padding:0 .4em;color:#7f7f7f">1
</span></code></pre></td><td style=vertical-align:top;padding:0;margin:0;border:0;width:100%><pre tabindex=0 style=background-color:#fff;-moz-tab-size:2;-o-tab-size:2;tab-size:2><code class=language-shell data-lang=shell><span style=display:flex><span>bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster ./examples/jars/spark-examples_2.12-3.0.0.jar <span style=color:#1c01ce>10</span>
</span></span></code></pre></td></tr></table></div></div></div></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2023 The Herb 保留所有权利</small>
<small class=ml-1><a href=# target=_blank rel=noopener>隐私政策</a></small><p class=mt-2><a href=/about/>Herbdocs</a></p></div></div></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.91798a335c881f1b6b805085ba4aa22d1dbd2b0b18d105d05189fa104ddae350.js integrity="sha256-kXmKM1yIHxtrgFCFukqiLR29KwsY0QXQUYn6EE3a41A=" crossorigin=anonymous></script></body></html>